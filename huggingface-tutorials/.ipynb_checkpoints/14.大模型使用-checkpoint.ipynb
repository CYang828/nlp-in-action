{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ–‡æœ¬\n",
    "\n",
    "LLMsï¼Œå³å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ˜¯æ–‡æœ¬ç”ŸæˆèƒŒåçš„å…³é”®ç»„ä»¶ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå®ƒä»¬ç”±å¤§å‹é¢„è®­ç»ƒçš„Transformeræ¨¡å‹ç»„æˆï¼Œç»è¿‡è®­ç»ƒä»¥é¢„æµ‹ç»™å®šä¸€äº›è¾“å…¥æ–‡æœ¬æ—¶çš„ä¸‹ä¸€ä¸ªå•è¯ï¼ˆæˆ–æ›´å‡†ç¡®åœ°è¯´ï¼Œæ ‡è®°ï¼‰ã€‚ç”±äºå®ƒä»¬ä¸€æ¬¡é¢„æµ‹ä¸€ä¸ªæ ‡è®°ï¼Œå› æ­¤æ‚¨éœ€è¦åšä¸€äº›æ›´å¤æ‚çš„äº‹æƒ…æ¥ç”Ÿæˆæ–°çš„å¥å­ï¼Œè€Œä¸ä»…ä»…æ˜¯è°ƒç”¨æ¨¡å‹ â€” æ‚¨éœ€è¦è¿›è¡Œè‡ªå›å½’ç”Ÿæˆã€‚\n",
    "\n",
    "è‡ªå›å½’ç”Ÿæˆæ˜¯åœ¨æ¨æ–­æ—¶é€šè¿‡åå¤è°ƒç”¨æ¨¡å‹ä»¥å…¶è‡ªå·±ç”Ÿæˆçš„è¾“å‡ºæ¥è¿›è¡Œçš„è¿‡ç¨‹ï¼Œç»™å®šä¸€äº›åˆå§‹è¾“å…¥ã€‚åœ¨ğŸ¤— Transformersä¸­ï¼Œè¿™ç”±generate()æ–¹æ³•å¤„ç†ï¼Œé€‚ç”¨äºæ‰€æœ‰å…·æœ‰ç”Ÿæˆèƒ½åŠ›çš„æ¨¡å‹ã€‚\n",
    "\n",
    "æœ¬æ•™ç¨‹å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š\n",
    "\n",
    "- ä½¿ç”¨LLMç”Ÿæˆæ–‡æœ¬\n",
    "- é¿å…å¸¸è§é™·é˜±\n",
    "- ä¸‹ä¸€æ­¥ï¼Œå¸®åŠ©æ‚¨å……åˆ†åˆ©ç”¨LLM\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š\n",
    "\n",
    "```\n",
    "pip install transformers bitsandbytes>=0.39.0 -q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç”Ÿæˆæ–‡æœ¬\n",
    "\n",
    "\n",
    "ä¸ºå› æœè¯­è¨€å»ºæ¨¡è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å°†æ–‡æœ¬æ ‡è®°åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸‹ä¸€ä¸ªæ ‡è®°çš„æ¦‚ç‡åˆ†å¸ƒã€‚\n",
    "\n",
    "\n",
    "<video id=\"video\" controls=\"\" preload=\"none\" poster=\"å°é¢\">\n",
    "      <source id=\"mp4\" src=\"http://stream.rarelimiting.com/gif_1_1080p.mov\" type=\"video/mp4\">\n",
    "</videos>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMè‡ªå›å½’ç”Ÿæˆçš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯å¦‚ä½•ä»æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œä»»ä½•æ–¹æ³•éƒ½å¯ä»¥ï¼Œåªè¦æœ€ç»ˆå¾—åˆ°ä¸‹ä¸€æ¬¡è¿­ä»£çš„æ ‡è®°ã€‚è¿™æ„å‘³ç€å¯ä»¥ç®€å•åœ°ä»æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©æœ€å¯èƒ½çš„æ ‡è®°ï¼Œä¹Ÿå¯ä»¥åœ¨ä»ç»“æœåˆ†å¸ƒä¸­æŠ½æ ·ä¹‹å‰åº”ç”¨ä¸€ç³»åˆ—å¤æ‚çš„è½¬æ¢ã€‚\n",
    "\n",
    "<video id=\"video\" controls=\"\" preload=\"none\" poster=\"å°é¢\">\n",
    "      <source id=\"mp4\" src=\"http://stream.rarelimiting.com/gif_2_1080p.mov\" type=\"video/mp4\">\n",
    "</videos>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šè¿°è¿‡ç¨‹ä¼šé‡å¤è¿›è¡Œï¼Œç›´åˆ°è¾¾åˆ°æŸä¸ªåœæ­¢æ¡ä»¶ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œåœæ­¢æ¡ä»¶ç”±æ¨¡å‹å†³å®šï¼Œæ¨¡å‹åº”å­¦ä¼šä½•æ—¶è¾“å‡ºä¸€ä¸ªåºåˆ—ç»“æŸï¼ˆEOSï¼‰æ ‡è®°ã€‚å¦‚æœä¸æ˜¯è¿™ç§æƒ…å†µï¼Œç”Ÿæˆä¼šåœ¨è¾¾åˆ°é¢„å®šä¹‰çš„æœ€å¤§é•¿åº¦æ—¶åœæ­¢ã€‚\n",
    "\n",
    "æ­£ç¡®è®¾ç½®æ ‡è®°é€‰æ‹©æ­¥éª¤å’Œåœæ­¢æ¡ä»¶å¯¹ä½¿æ¨¡å‹æŒ‰é¢„æœŸè¡Œä¸ºè‡³å…³é‡è¦ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ºæ¯ä¸ªæ¨¡å‹å…³è”ä¸€ä¸ªGenerationConfigæ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«è‰¯å¥½çš„é»˜è®¤ç”Ÿæˆå‚æ•°è®¾ç½®ï¼Œå¹¶ä¸æ‚¨çš„æ¨¡å‹ä¸€èµ·åŠ è½½ã€‚\n",
    "\n",
    "è®©æˆ‘ä»¬æ¥è°ˆè°ˆä»£ç ï¼\n",
    "\n",
    ">> å¦‚æœæ‚¨å¯¹åŸºæœ¬LLMç”¨æ³•æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬çš„é«˜çº§Pipelineæ¥å£æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚ç„¶è€Œï¼ŒLLMé€šå¸¸éœ€è¦é«˜çº§åŠŸèƒ½ï¼Œå¦‚é‡åŒ–å’Œå¯¹æ ‡è®°é€‰æ‹©æ­¥éª¤çš„ç²¾ç»†æ§åˆ¶ï¼Œæœ€å¥½é€šè¿‡generate()æ¥å®ç°ã€‚LLMçš„è‡ªå›å½’ç”Ÿæˆä¹Ÿéœ€è¦å¤§é‡èµ„æºï¼Œå¹¶ä¸”åº”åœ¨GPUä¸Šæ‰§è¡Œä»¥è·å¾—è¶³å¤Ÿçš„ååé‡ã€‚\n",
    "\n",
    "é¦–å…ˆï¼Œæ‚¨éœ€è¦åŠ è½½æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‚¨ä¼šæ³¨æ„åˆ°from_pretrainedè°ƒç”¨ä¸­æœ‰ä¸¤ä¸ªæ ‡å¿—ï¼š\n",
    "\n",
    "- device_map ç¡®ä¿æ¨¡å‹è¢«ç§»åŠ¨åˆ°æ‚¨çš„GPU\n",
    "- load_in_4bit åº”ç”¨4ä½åŠ¨æ€é‡åŒ–ä»¥å¤§å¹…å‡å°‘èµ„æºéœ€æ±‚\n",
    "\n",
    "è¿˜æœ‰å…¶ä»–åˆå§‹åŒ–æ¨¡å‹çš„æ–¹æ³•ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„åŸºå‡†ï¼Œç”¨äºå¼€å§‹ä½¿ç”¨LLMã€‚\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦ä½¿ç”¨åˆ†è¯å™¨é¢„å¤„ç†æ–‡æœ¬è¾“å…¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n",
    "model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_inputså˜é‡ä¿å­˜äº†åˆ†è¯åçš„æ–‡æœ¬è¾“å…¥ï¼Œä»¥åŠæ³¨æ„åŠ›æ©ç ã€‚è™½ç„¶generate()ä¼šå°½åŠ›æ¨æ–­æ³¨æ„åŠ›æ©ç ï¼Œä½†æˆ‘ä»¬å»ºè®®å°½å¯èƒ½ä¼ é€’å®ƒä»¥è·å¾—æœ€ä½³ç»“æœã€‚\n",
    "\n",
    "åœ¨å¯¹è¾“å…¥è¿›è¡Œåˆ†è¯åï¼Œæ‚¨å¯ä»¥è°ƒç”¨generate()æ–¹æ³•è¿”å›ç”Ÿæˆçš„æ ‡è®°ã€‚ç„¶ååº”å°†ç”Ÿæˆçš„æ ‡è®°è½¬æ¢ä¸ºæ–‡æœ¬åæ‰“å°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åï¼Œæ‚¨ä¸éœ€è¦ä¸€æ¬¡å¤„ç†ä¸€ä¸ªåºåˆ—ï¼æ‚¨å¯ä»¥æ‰¹é‡å¤„ç†è¾“å…¥ï¼Œè¿™å°†å¤§å¤§æé«˜ååé‡ï¼ŒåŒæ—¶å»¶è¿Ÿå’Œå†…å­˜æˆæœ¬è¾ƒå°ã€‚æ‚¨åªéœ€è¦ç¡®ä¿æ­£ç¡®å¡«å……æ‚¨çš„è¾“å…¥å³å¯ï¼ˆä¸‹é¢ä¼šè¯¦ç»†ä»‹ç»ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°±æ˜¯è¿™æ ·ï¼å‡ è¡Œä»£ç å°±å¯ä»¥åˆ©ç”¨LLMçš„å¼ºå¤§åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¸¸è§é™·é˜±\n",
    "\n",
    "æœ‰è®¸å¤šç”Ÿæˆç­–ç•¥ï¼Œæœ‰æ—¶é»˜è®¤å€¼å¯èƒ½ä¸é€‚åˆæ‚¨çš„ç”¨ä¾‹ã€‚å¦‚æœæ‚¨çš„è¾“å‡ºä¸æ‚¨çš„é¢„æœŸä¸ä¸€è‡´ï¼Œæˆ‘ä»¬å·²ç»åˆ›å»ºäº†ä¸€ä¸ªå¸¸è§é™·é˜±åˆ—è¡¨ä»¥åŠå¦‚ä½•é¿å…å®ƒä»¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç”Ÿæˆçš„è¾“å‡ºè¿‡çŸ­/è¿‡é•¿\n",
    "\n",
    "å¦‚æœåœ¨GenerationConfigæ–‡ä»¶ä¸­æœªæŒ‡å®šï¼Œgenerateé»˜è®¤è¿”å›æœ€å¤š20ä¸ªæ ‡è®°ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®åœ¨generateè°ƒç”¨ä¸­æ‰‹åŠ¨è®¾ç½®max_new_tokensï¼Œä»¥æ§åˆ¶å®ƒå¯ä»¥è¿”å›çš„æœ€å¤§æ–°æ ‡è®°æ•°ã€‚è¯·è®°ä½ï¼ŒLLMï¼ˆæ›´å‡†ç¡®åœ°è¯´ï¼Œä»…è§£ç å™¨æ¨¡å‹ï¼‰è¿˜ä¼šå°†è¾“å…¥æç¤ºä½œä¸ºè¾“å‡ºçš„ä¸€éƒ¨åˆ†è¿”å›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# By default, the output will contain up to 20 tokens\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# Setting `max_new_tokens` allows you to control the maximum length\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=50)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é”™è¯¯çš„ç”Ÿæˆæ¨¡å¼\n",
    "\n",
    "é»˜è®¤æƒ…å†µä¸‹ï¼Œé™¤éåœ¨GenerationConfigæ–‡ä»¶ä¸­æŒ‡å®šï¼Œgenerateåœ¨æ¯æ¬¡è¿­ä»£ä¸­é€‰æ‹©æœ€å¯èƒ½çš„æ ‡è®°ï¼ˆè´ªå©ªè§£ç ï¼‰ã€‚æ ¹æ®æ‚¨çš„ä»»åŠ¡ï¼Œè¿™å¯èƒ½æ˜¯ä¸å¯å–çš„ï¼›åƒèŠå¤©æœºå™¨äººæˆ–å†™ä½œæ–‡ç« è¿™æ ·çš„åˆ›é€ æ€§ä»»åŠ¡å—ç›ŠäºæŠ½æ ·ã€‚å¦ä¸€æ–¹é¢ï¼ŒåƒéŸ³é¢‘è½¬å½•æˆ–ç¿»è¯‘è¿™æ ·çš„è¾“å…¥é©±åŠ¨ä»»åŠ¡å—ç›Šäºè´ªå©ªè§£ç ã€‚é€šè¿‡è®¾ç½®do_sample=Trueå¯ç”¨æŠ½æ ·ï¼Œæ‚¨å¯ä»¥åœ¨è¿™ç¯‡åšæ–‡ä¸­äº†è§£æ›´å¤šå…³äºè¿™ä¸ªä¸»é¢˜çš„å†…å®¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed or reproducibility -- you don't need this unless you want full reproducibility\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# LLM + greedy decoding = repetitive, boring output\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# With sampling, the output becomes more creative!\n",
    "generated_ids = model.generate(**model_inputs, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é”™è¯¯çš„å¡«å……æ–¹å¼\n",
    "\n",
    "LLMæ˜¯ä»…è§£ç å™¨æ¶æ„ï¼Œæ„å‘³ç€å®ƒä»¬ä¼šç»§ç»­è¿­ä»£æ‚¨çš„è¾“å…¥æç¤ºã€‚å¦‚æœæ‚¨çš„è¾“å…¥é•¿åº¦ä¸ç›¸åŒï¼Œå®ƒä»¬éœ€è¦è¢«å¡«å……ã€‚ç”±äºLLMæ²¡æœ‰ç»è¿‡è®­ç»ƒä»¥ä»å¡«å……æ ‡è®°ç»§ç»­ï¼Œå› æ­¤æ‚¨çš„è¾“å…¥éœ€è¦è¿›è¡Œå·¦å¡«å……ã€‚ç¡®ä¿æ‚¨è¿˜è®°å¾—ä¼ é€’æ³¨æ„åŠ›æ©ç ç»™generateï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# The tokenizer initialized above has right-padding active by default: the 1st sequence,\n",
    "# which is shorter, has padding on the right side. Generation fails to capture the logic.\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# With left-padding, it works as expected!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é”™è¯¯çš„æç¤º\n",
    "\n",
    "ä¸€äº›æ¨¡å‹å’Œä»»åŠ¡æœŸæœ›ç‰¹å®šçš„è¾“å…¥æç¤ºæ ¼å¼æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚å¦‚æœæœªåº”ç”¨æ­¤æ ¼å¼ï¼Œæ‚¨å°†ä¼šé‡åˆ°æ€§èƒ½ä¸‹é™ï¼šæ¨¡å‹å¯èƒ½ä¼šè¿è¡Œï¼Œä½†æ•ˆæœä¸å¦‚æŒ‰ç…§é¢„æœŸæç¤ºã€‚æœ‰å…³æç¤ºçš„æ›´å¤šä¿¡æ¯ï¼ŒåŒ…æ‹¬å“ªäº›æ¨¡å‹å’Œä»»åŠ¡éœ€è¦æ³¨æ„ï¼Œå¯åœ¨æ­¤æŒ‡å—ä¸­æ‰¾åˆ°ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä½¿ç”¨èŠå¤©æ¨¡æ¿çš„èŠå¤©LLMç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", load_in_4bit=True\n",
    ")\n",
    "set_seed(0)\n",
    "prompt = \"\"\"How many helicopters can a human eat in one sitting? Reply as a thug.\"\"\"\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "input_length = model_inputs.input_ids.shape[1]\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=20)\n",
    "print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\n",
    "# Oh no, it did not follow our instruction to reply as a thug! Let's see what happens when we write\n",
    "# a better prompt and use the right template for this model (through `tokenizer.apply_chat_template`)\n",
    "\n",
    "set_seed(0)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a thug\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "input_length = model_inputs.shape[1]\n",
    "generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)\n",
    "print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\n",
    "# As we can see, it followed a proper thug style ğŸ˜"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
