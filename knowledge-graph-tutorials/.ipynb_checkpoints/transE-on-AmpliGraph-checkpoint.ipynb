{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Example\n",
    "In this example we show how to load a benchmark dataset, how to train a model on it and which are the different types of evaluation protocols that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ampligraph\n",
    "# Benchmark datasets are under ampligraph.datasets module\n",
    "from ampligraph.datasets import load_fb15k_237\n",
    "# load fb15k-237 dataset\n",
    "dataset = load_fb15k_237()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 7s 233ms/step - loss: 36773.3906\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 6s 215ms/step - loss: 22626.3574\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 6s 202ms/step - loss: 17343.5254\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 6s 198ms/step - loss: 14640.1602\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 6s 223ms/step - loss: 13013.9121\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 6s 202ms/step - loss: 11937.1406\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 6s 194ms/step - loss: 11176.6133\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 6s 213ms/step - loss: 10612.2920\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 6s 196ms/step - loss: 10177.2930\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 6s 200ms/step - loss: 9833.4297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3836fd1b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the KGE model\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
    "\n",
    "# you can continue training from where you left after restoring the model\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./transe_train_logs')\n",
    "\n",
    "# create the model with transe scoring function\n",
    "model = ScoringBasedEmbeddingModel(eta=5,\n",
    "                                   k=300,\n",
    "                                   scoring_type='TransE')\n",
    "\n",
    "# you can either use optimizers/regularizers/loss/initializers with default values or you can \n",
    "# import it and customize the hyperparameters and pass it to compile\n",
    "\n",
    "# Let's create an adam optimizer with customized learning rate =0.005\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "# Let's compile the model with self_advarsarial loss of default parameters\n",
    "model.compile(optimizer=adam, loss='self_adversarial')\n",
    "\n",
    "# fit the model to data.\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             callbacks=[tensorboard_callback])\n",
    "\n",
    "# the training can be visualised using the following command:\n",
    "# tensorboard --logdir='./transe_train_logs' --port=8891 \n",
    "# open the browser and go to the following URL: http://127.0.0.1:8891/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.266111 , -2.095364 , -2.289988 , -3.8879156, -4.619501 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(dataset['test'][:5], \n",
    "                       batch_size=100)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model (without filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject and object (s,o) corruption\n",
    "This evaluation protocol consists in creating corrupted triples via the corruption of both subject and object of existing triples. This is considered the standard protocol for evaluation in Knowledge Graph Embedding models literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 46s 223ms/step\n",
      "MR: 472.3101575496624\n",
      "MRR: 0.08900155667769008\n",
      "hits@1: 0.0\n",
      "hits@10: 0.2428319796457579\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dataset['test'],     # test set\n",
    "                       batch_size=100,      # evaluation batch size\n",
    "                       corrupt_side='s,o'   # sides to corrupt for scoring and ranking\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object corruption\n",
    "Corruptions are generated by changing only the object of triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 25s 123ms/step\n",
      "MR: 268.11287797240436\n",
      "MRR: 0.12974471587250763\n",
      "hits@1: 0.0\n",
      "hits@10: 0.3512085331245719\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dataset['test'], \n",
    "                       batch_size=100, \n",
    "                       corrupt_side='o' # corrupt only the object\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject corruption\n",
    "Corruptions are generated by changing only the subject of triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 27s 129ms/step\n",
      "MR: 676.5074371269204\n",
      "MRR: 0.048258397482872535\n",
      "hits@1: 0.0\n",
      "hits@10: 0.13445542616694392\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dataset['test'], \n",
    "                       batch_size=100, \n",
    "                       corrupt_side='s' # corrupt only the subject\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Filters\n",
    "Triples specified inside the filter are removed from the corruptions that are generated to avoid the creation of false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 929s 5s/step\n",
      "MR: 364.22042274195127\n",
      "MRR: 0.19061439893688528\n",
      "hits@1: 0.12329973578628045\n",
      "hits@10: 0.32199823857520304\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dataset['test'], \n",
    "                       batch_size=100, \n",
    "                       corrupt_side='s,o', # corrupt both subject and object\n",
    "                       use_filter={'train':dataset['train'], # Filter to be used for evaluation\n",
    "                                   'valid':dataset['valid'],\n",
    "                                   'test':dataset['test']}\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation using a subset of entities for corruption\n",
    "Specify a subset of entities that are used for corrupting, depending on the evaluation strategy chosen, either the subject, the object or both. Notice that, despite not being the standard evaluation protocol, using a subset of entities can generate more meaningful corruptions and also reduce a lot the computational overhead caused by sampling corruptions among all the entities in the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get all the month present in training\n",
    "months = set(dataset['train'][\n",
    "    dataset['train'][:, 1] == \n",
    "        '/travel/travel_destination/climate./travel/travel_destination_monthly_climate/month'][:, 2])\n",
    "len(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider we are evaluating the below test set which is specific to one predicate\n",
    "# This predicate tells the best time of the year(o) to visit a destination (s)\n",
    "dest_month_test_triples = dataset['test'][\n",
    "    dataset['test'][:, 1] ==\n",
    "        '/travel/travel_destination/climate./travel/travel_destination_monthly_climate/month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 1s/step\n",
      "MR: 1.0833333333333333\n",
      "MRR: 0.9861111111111112\n",
      "hits@1: 0.9833333333333333\n",
      "hits@10: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Let's say we want to evaluate this test set by corrupting the object with only months.\n",
    "# we can pass the months as entities_subset and generate corruptions only using this subset \n",
    "# instead of all entities in the graph\n",
    "# This approach is very useful when the graph size is big and/or \n",
    "# when our hypothesis belongs to a specific predicate type\n",
    "# When graph size is big we can randomly sample fixed number of small subset of entities and use it as corruption\n",
    "\n",
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dest_month_test_triples, \n",
    "                       batch_size=100, \n",
    "                       corrupt_side='o', # corrupt only the object\n",
    "                       entities_subset=months,\n",
    "                       use_filter={'train':dataset['train'], # Filter to be used for evaluation\n",
    "                                   'valid':dataset['valid'],\n",
    "                                   'test':dataset['test']}\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.utils import create_tensorboard_visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, \n",
    "                                  entities_subset=['/m/027rn', '/m/06cx9', '/m/017dcd', '/m/06v8s0', '/m/07s9rl0'], \n",
    "                                  labels=['ent1', 'ent2', 'ent3', 'ent4', 'ent5'],\n",
    "                                  loc = './selected_subset_embeddings_vis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, \n",
    "                                  entities_subset='all',\n",
    "                                  loc = './full_embeddings_vis')\n",
    "\n",
    "# the embeddings can be visualised uncommenting the following command:\n",
    "# ! tensorboard --logdir='./full_embeddings_vis' --port=8891 \n",
    "# open the browser and go to the following URL: http://127.0.0.1:8891/#projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping\n",
    "The following example shows how to use early stopping while training a model and how to create checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ampligraph\n",
    "# Benchmark datasets are under ampligraph.datasets module\n",
    "from ampligraph.datasets import load_fb15k_237\n",
    "# load fb15k-237 dataset\n",
    "dataset = load_fb15k_237()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/29 [==============================] - 3s 94ms/step - loss: 6622.9019\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 6493.6064"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:22.984591: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 262ms/step\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 6493.6064 - val_mrr: 0.0684 - val_mr: 3258.6080 - val_hits@1: 0.0000e+00 - val_hits@10: 0.1790 - val_hits@100: 0.3466\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 6365.1802\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 6234.0171"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:26.191442: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 219ms/step\n",
      "29/29 [==============================] - 2s 69ms/step - loss: 6234.0171 - val_mrr: 0.0809 - val_mr: 1924.0540 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2102 - val_hits@100: 0.4006\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 2s 59ms/step - loss: 6101.5239\n",
      "Epoch 6/100\n",
      "26/29 [=========================>....] - ETA: 0s - loss: 6003.9404"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:29.466724: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 319ms/step\n",
      "29/29 [==============================] - 2s 55ms/step - loss: 5966.6636 - val_mrr: 0.0777 - val_mr: 1346.6420 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2188 - val_hits@100: 0.4347\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 1s 27ms/step - loss: 5829.4419\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 5690.5356"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:32.519336: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 215ms/step\n",
      "29/29 [==============================] - 2s 72ms/step - loss: 5690.5356 - val_mrr: 0.0793 - val_mr: 1060.0114 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2216 - val_hits@100: 0.4659\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 2s 56ms/step - loss: 5553.0103\n",
      "Epoch 10/100\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 5434.2549"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:35.457800: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 235ms/step\n",
      "29/29 [==============================] - 1s 46ms/step - loss: 5418.0444 - val_mrr: 0.0837 - val_mr: 901.1193 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2216 - val_hits@100: 0.4830\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 1s 26ms/step - loss: 5286.5752\n",
      "Epoch 12/100\n",
      "      2/Unknown - 0s 159ms/step===>..] - ETA: 0s - loss: 5171.8193"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:39.521803: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 147ms/step\n",
      "29/29 [==============================] - 3s 102ms/step - loss: 5158.7935 - val_mrr: 0.0832 - val_mr: 841.9688 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2216 - val_hits@100: 0.4858\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 5035.1396\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 107ms/steposs: 4926.96\n",
      "29/29 [==============================] - 1s 24ms/step - loss: 4916.2104 - val_mrr: 0.0839 - val_mr: 815.3011 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2330 - val_hits@100: 0.4972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:40.702638: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "29/29 [==============================] - 1s 26ms/step - loss: 4801.8057\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 4691.9980"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:42.851130: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 249ms/step\n",
      "29/29 [==============================] - 2s 61ms/step - loss: 4691.9980 - val_mrr: 0.0870 - val_mr: 777.9716 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2301 - val_hits@100: 0.5114\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 4587.6689\n",
      "Epoch 18/100\n",
      "27/29 [==========================>...] - ETA: 0s - loss: 4499.3130"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:45.612853: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 218ms/step\n",
      "29/29 [==============================] - 2s 69ms/step - loss: 4488.2705 - val_mrr: 0.0833 - val_mr: 755.6392 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2216 - val_hits@100: 0.5199\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 1s 29ms/step - loss: 4393.5967\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 4303.0571"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:47.841319: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 234ms/step\n",
      "29/29 [==============================] - 1s 48ms/step - loss: 4303.0571 - val_mrr: 0.0827 - val_mr: 739.2500 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2216 - val_hits@100: 0.5227\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 4217.0073\n",
      "Epoch 22/100\n",
      "      2/Unknown - 0s 152ms/step===>..] - ETA: 0s - loss: 4140.2354"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:50.470017: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 165ms/step\n",
      "29/29 [==============================] - 2s 64ms/step - loss: 4134.3545 - val_mrr: 0.0839 - val_mr: 737.0227 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2244 - val_hits@100: 0.5170\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 22: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28a6d6da0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the KGE model\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
    "\n",
    "# create the model with transe scoring function\n",
    "model = ScoringBasedEmbeddingModel(eta=1, \n",
    "                                     k=10,\n",
    "                                     scoring_type='TransE')\n",
    "\n",
    "\n",
    "# compile the model with loss and optimizer\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "# Use this for checkpoints at regular intervals\n",
    "#checkpoint = tf.keras.callbacks.ModelCheckpoint('./chkpt_transe', monitor='val_mrr', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Use this for early stopping\n",
    "checkpoint = tf.keras.callbacks.EarlyStopping(monitor=\"val_mrr\",            # which metrics to monitor\n",
    "                                              patience=3,                   # If the monitored metric doesnt improve \n",
    "                                                                            # for these many checks the model early stops\n",
    "                                              verbose=1,                    # verbosity\n",
    "                                              mode=\"max\",                   # how to compare the monitored metrics. \n",
    "                                                                            # max - means higher is better\n",
    "                                              restore_best_weights=True)    # restore the weights with best value\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "model.fit(dataset['train'],\n",
    "          batch_size=10000,\n",
    "          epochs=100,\n",
    "          validation_freq=2,                            # Epochs to elapse before next evaluation\n",
    "          validation_batch_size=100,                    \n",
    "          validation_burn_in=1,                         # Epoch to start the validation process\n",
    "          validation_data = dataset['valid'][::100],\n",
    "          callbacks=[checkpoint])                       # Pass the callback to the fit function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 616s 3s/step\n",
      "MR: 600.0142626480086\n",
      "MRR: 0.18888812773966743\n",
      "hits@1: 0.1267491926803014\n",
      "hits@10: 0.3130687934240141\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dataset['test'], # test set\n",
    "                       batch_size=100, # evaluation batch size\n",
    "                       corrupt_side='s,o', \n",
    "                       use_filter={'train':dataset['train'], # Filter to be used for evaluation\n",
    "                                   'valid':dataset['valid'],\n",
    "                                   'test':dataset['test']}\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using ModelCheckpoint then we can restore the checkpoints using restore model\n",
    "# from ampligraph.utils import restore_model\n",
    "# model = restore_model('chkpt_transe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ampligraph\n",
    "# Benchmark datasets are under ampligraph.datasets module\n",
    "from ampligraph.datasets import load_fb15k_237\n",
    "# load fb15k-237 dataset\n",
    "dataset = load_fb15k_237()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 3s 101ms/step - loss: 6625.2280\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 1s 418ms/steposs: 6505.03\n",
      "29/29 [==============================] - 3s 92ms/step - loss: 6505.0376 - val_mrr: 0.0576 - val_mr: 3106.6989 - val_hits@1: 0.0000e+00 - val_hits@10: 0.1562 - val_hits@100: 0.3352\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 49ms/step - loss: 6386.7886\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 101ms/steposs: 6267.646\n",
      "29/29 [==============================] - 2s 63ms/step - loss: 6267.6465 - val_mrr: 0.0673 - val_mr: 2225.0398 - val_hits@1: 0.0000e+00 - val_hits@10: 0.1903 - val_hits@100: 0.3835\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 47ms/step - loss: 6149.4019\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 118ms/steposs: 6029.96\n",
      "29/29 [==============================] - 2s 63ms/step - loss: 6029.9697 - val_mrr: 0.0765 - val_mr: 1584.9233 - val_hits@1: 0.0000e+00 - val_hits@10: 0.1989 - val_hits@100: 0.4119\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 51ms/step - loss: 5910.1436\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 126ms/steposs: 5814.78\n",
      "29/29 [==============================] - 2s 59ms/step - loss: 5789.8135 - val_mrr: 0.0732 - val_mr: 1316.3835 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2045 - val_hits@100: 0.4489\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 43ms/step - loss: 5670.5464\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 1s 186ms/steposs: 5569.24\n",
      "29/29 [==============================] - 2s 70ms/step - loss: 5552.7808 - val_mrr: 0.0728 - val_mr: 1146.0284 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2131 - val_hits@100: 0.4801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2931fd2a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the KGE model\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
    "\n",
    "# create the model with transe scoring function\n",
    "model = ScoringBasedEmbeddingModel(eta=1, \n",
    "                                     k=10,\n",
    "                                     scoring_type='TransE')\n",
    "\n",
    "\n",
    "# compile the model with loss and optimizer\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "# Use this for checkpoints at regular intervals\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('./chkpt1_transe', monitor='val_mrr', verbose=0, \n",
    "                                                save_best_only=True, mode='min')\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "model.fit(dataset['train'],\n",
    "          batch_size=10000,\n",
    "          epochs=10,\n",
    "          validation_freq=2,\n",
    "          validation_batch_size=100,\n",
    "          validation_burn_in=2,\n",
    "          validation_data = dataset['valid'][::100],\n",
    "          callbacks=[checkpoint])     # Pass the callback to the fit function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 407s 2s/step\n",
      "MR: 921.3923084450533\n",
      "MRR: 0.17730360323410402\n",
      "hits@1: 0.12151384675604267\n",
      "hits@10: 0.2851551032390645\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dataset['test'], # test set\n",
    "                       batch_size=100, # evaluation batch size\n",
    "                       corrupt_side='s,o', \n",
    "                       use_filter={'train':dataset['train'], # Filter to be used for evaluation\n",
    "                                   'valid':dataset['valid'],\n",
    "                                   'test':dataset['test']}\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.utils import save_model\n",
    "# explictly save the model\n",
    "save_model(model, 'saved_model_transE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model does not include a db file. Skipping.\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.utils import restore_model\n",
    "\n",
    "# restore saved models or checkpoints\n",
    "model = restore_model('saved_model_transE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 254s 1s/step\n",
      "MR: 921.3923084450533\n",
      "MRR: 0.17730360323410402\n",
      "hits@1: 0.12151384675604267\n",
      "hits@10: 0.2851551032390645\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dataset['test'], # test set\n",
    "                       batch_size=100, # evaluation batch size\n",
    "                       corrupt_side='s,o', \n",
    "                       use_filter={'train':dataset['train'], # Filter to be used for evaluation\n",
    "                                   'valid':dataset['valid'],\n",
    "                                   'test':dataset['test']}\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioned Training\n",
    "The following example shows how to train a model with partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import numpy as np\n",
    "import ampligraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and predict scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 848.0Bytes, after: 4.3447MB, consumed: 4.3439MB; exec time: 29.242s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alberto.bernardi/miniforge3/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "2023-02-08 16:47:49.873938: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - 3s 257ms/step - loss: 8690.4297\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 8642.6641\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 8588.0459\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 8524.0557\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 8454.0918\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 2s 167ms/step - loss: 8382.4600\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 8313.3584\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 8245.1660\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 2s 160ms/step - loss: 8180.5093\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 8118.5229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28f31d630>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the KGE model\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
    "\n",
    "PATH_TO_DATASET = 'your/path/to/dataset/'\n",
    "\n",
    "# create the model with transe scoring function\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                               k=50, \n",
    "                                               scoring_type='TransE')\n",
    "partitioned_model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "# Here we have specified the path of the input file\n",
    "# you can also load using default dataloaders load_fb15k_237() and pass numpy array inputs\n",
    "partitioned_model.fit(PATH_TO_DATASET + 'wn18RR/train.txt',\n",
    "                      batch_size=10000, \n",
    "                      partitioning_k=3, # set flag to partition the inputs\n",
    "                      epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "9/9 [==============================] - 14s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20079.140731874144, 0.011132840015629617, 0.0, 0.03625170998632011, 2924)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unfiltered evaluation\n",
    "ranks = partitioned_model.evaluate(PATH_TO_DATASET + 'wn18RR/test.txt', \n",
    "                                   batch_size=400)\n",
    "\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "9/9 [==============================] - 68s 8s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20066.594562243503,\n",
       " 0.01583735697421522,\n",
       " 0.005471956224350205,\n",
       " 0.038132694938440494,\n",
       " 2924)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtered evaluation\n",
    "ranks = partitioned_model.evaluate(PATH_TO_DATASET + 'wn18RR/test.txt', \n",
    "                        batch_size=400,\n",
    "                        corrupt_side='s,o',\n",
    "                        use_filter={'train': PATH_TO_DATASET + 'wn18RR/train.txt',\n",
    "                                    'valid': PATH_TO_DATASET + 'wn18RR/valid.txt',\n",
    "                                    'test': PATH_TO_DATASET + 'wn18RR/test.txt'})\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path ./partitioned_model already exists. This save operation will overwrite the model                 at the specified path.\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.utils import save_model\n",
    "save_model(model=partitioned_model, model_name_path='./partitioned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model does not include a db file. Skipping.\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.utils import restore_model\n",
    "model = restore_model('./partitioned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "9/9 [==============================] - 15s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20079.140731874144, 0.011132840015629617, 0.0, 0.03625170998632011, 2924)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unfiltered evaluation\n",
    "ranks = model.evaluate(PATH_TO_DATASET + 'wn18RR/test.txt',\n",
    "                       batch_size=400)\n",
    "\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "9/9 [==============================] - 73s 8s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20066.594562243503,\n",
       " 0.01583735697421522,\n",
       " 0.005471956224350205,\n",
       " 0.038132694938440494,\n",
       " 2924)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = model.evaluate(PATH_TO_DATASET + 'wn18RR/test.txt', \n",
    "                        batch_size=400,\n",
    "                        corrupt_side='s,o',\n",
    "                        use_filter={'train': PATH_TO_DATASET + 'wn18RR/train.txt',\n",
    "                                    'valid': PATH_TO_DATASET + 'wn18RR/valid.txt',\n",
    "                                    'test': PATH_TO_DATASET + 'wn18RR/test.txt'})\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e69f3670cdad0193847aaa0b77be56c05c951fcbdd384ff882dde0464f4de76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
