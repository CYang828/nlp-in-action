{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AmpliGraph\n",
    "\n",
    "## 安装\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* Linux, macOS, Windows\n",
    "* Python ≥ 3.8\n",
    "\n",
    "### 创建虚拟环境\n",
    "\n",
    "为了为安装AmpliGraph准备一个虚拟环境，任何选项都可以；这里我们将提供使用`Conda`的指令。\n",
    "\n",
    "```\n",
    "conda create --name ampligraph python=3.8\n",
    "source activate ampligraph\n",
    "```\n",
    "\n",
    "完成后，我们可以继续安装TensorFlow 2，可以通过`pip`或`conda`进行安装。\n",
    "\n",
    "```\n",
    "pip install \"tensorflow==2.9.0\"\n",
    "\n",
    "or \n",
    "\n",
    "conda install \"tensorflow==2.9.0\"\n",
    "```\n",
    "\n",
    "#### 为Mac OS M1芯片安装TensorFlow 2\n",
    "\n",
    "在Mac OS上安装搭载Apple芯片的TensorFlow 2时，我们建议使用conda环境。\n",
    "\n",
    "```\n",
    "conda create --name ampligraph python=3.8\n",
    "source activate ampligraph\n",
    "```\n",
    "\n",
    "创建并激活虚拟环境后，运行以下命令安装Tensorflow。\n",
    "\n",
    "```\n",
    "conda install -c apple tensorflow-deps\n",
    "pip install --user tensorflow-macos==2.9.0\n",
    "pip install --user tensorflow-metal==0.6\n",
    "```\n",
    "\n",
    "如果安装出现问题或需要进一步了解，请参考官方Apple开发者网站上的[Tensorflow插件页面](https://developer.apple.com/metal/tensorflow-plugin/)。\n",
    "\n",
    "### 安装AmpliGraph\n",
    "\n",
    "一旦Tensorflow安装完成，我们可以继续安装AmpliGraph。\n",
    "\n",
    "要安装最新稳定版本，请使用pip：\n",
    "\n",
    "```\n",
    "pip install ampligraph\n",
    "```\n",
    "\n",
    "要检查安装情况，请运行以下命令：\n",
    "\n",
    "```python\n",
    ">>> import ampligraph\n",
    ">>> ampligraph.__version__\n",
    "'2.1.0'\n",
    "```\n",
    "\n",
    "如果您想要最新的开发版本，可以从[GitHub](https://github.com/Accenture/AmpliGraph.git)克隆存储库，从源代码安装AmpliGraph并切换到`develop`分支。这样，您的本地工作副本将位于`develop`分支上的最新提交。\n",
    "\n",
    "\n",
    "```\n",
    "git clone https://github.com/Accenture/AmpliGraph.git\n",
    "cd AmpliGraph\n",
    "git checkout develop\n",
    "pip install -e .\n",
    "```\n",
    "请注意，上面的代码段以可编辑模式（`-e`）安装库。\n",
    "\n",
    "要检查安装情况，请运行以下命令：\n",
    "\n",
    "```python\n",
    ">>> import ampligraph\n",
    ">>> ampligraph.__version__\n",
    "'2.1-dev'\n",
    "```\n",
    "\n",
    "\n",
    "在这个示例中，我们展示如何加载基准数据集，如何在其上训练模型以及我们可以使用哪些不同类型的评估协议。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ampligraph\n",
    "# Benchmark datasets are under ampligraph.datasets module\n",
    "from ampligraph.datasets import load_fb15k_237\n",
    "# load fb15k-237 dataset\n",
    "dataset = load_fb15k_237()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 7s 233ms/step - loss: 36773.3906\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 6s 215ms/step - loss: 22626.3574\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 6s 202ms/step - loss: 17343.5254\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 6s 198ms/step - loss: 14640.1602\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 6s 223ms/step - loss: 13013.9121\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 6s 202ms/step - loss: 11937.1406\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 6s 194ms/step - loss: 11176.6133\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 6s 213ms/step - loss: 10612.2920\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 6s 196ms/step - loss: 10177.2930\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 6s 200ms/step - loss: 9833.4297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3836fd1b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the KGE model\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
    "\n",
    "# you can continue training from where you left after restoring the model\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./transe_train_logs')\n",
    "\n",
    "# create the model with transe scoring function\n",
    "model = ScoringBasedEmbeddingModel(eta=5,\n",
    "                                   k=300,\n",
    "                                   scoring_type='TransE')\n",
    "\n",
    "# you can either use optimizers/regularizers/loss/initializers with default values or you can \n",
    "# import it and customize the hyperparameters and pass it to compile\n",
    "\n",
    "# Let's create an adam optimizer with customized learning rate =0.005\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "# Let's compile the model with self_advarsarial loss of default parameters\n",
    "model.compile(optimizer=adam, loss='self_adversarial')\n",
    "\n",
    "# fit the model to data.\n",
    "model.fit(dataset['train'],\n",
    "             batch_size=10000,\n",
    "             epochs=10,\n",
    "             callbacks=[tensorboard_callback])\n",
    "\n",
    "# the training can be visualised using the following command:\n",
    "# tensorboard --logdir='./transe_train_logs' --port=8891 \n",
    "# open the browser and go to the following URL: http://127.0.0.1:8891/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.266111 , -2.095364 , -2.289988 , -3.8879156, -4.619501 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(dataset['test'][:5], \n",
    "                       batch_size=100)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model (without filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主语和宾语（s，o）破坏\n",
    "这种评估协议包括通过破坏现有三元组的主语和宾语来创建损坏的三元组。这被认为是知识图嵌入模型文献中的标准评估协议。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 46s 223ms/step\n",
      "MR: 472.3101575496624\n",
      "MRR: 0.08900155667769008\n",
      "hits@1: 0.0\n",
      "hits@10: 0.2428319796457579\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dataset['test'],     # test set\n",
    "                       batch_size=100,      # evaluation batch size\n",
    "                       corrupt_side='s,o'   # sides to corrupt for scoring and ranking\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 宾语破坏生成负样本\n",
    "通过仅更改三元组的宾语来生成损坏的三元组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 25s 123ms/step\n",
      "MR: 268.11287797240436\n",
      "MRR: 0.12974471587250763\n",
      "hits@1: 0.0\n",
      "hits@10: 0.3512085331245719\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dataset['test'], \n",
    "                       batch_size=100, \n",
    "                       corrupt_side='o' # corrupt only the object\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 27s 129ms/step\n",
      "MR: 676.5074371269204\n",
      "MRR: 0.048258397482872535\n",
      "hits@1: 0.0\n",
      "hits@10: 0.13445542616694392\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dataset['test'], \n",
    "                       batch_size=100, \n",
    "                       corrupt_side='s' # corrupt only the subject\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用过滤器进行评估\n",
    "在生成的损坏三元组中，指定在过滤器内的三元组将被移除，以避免产生错误的负例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 929s 5s/step\n",
      "MR: 364.22042274195127\n",
      "MRR: 0.19061439893688528\n",
      "hits@1: 0.12329973578628045\n",
      "hits@10: 0.32199823857520304\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dataset['test'], \n",
    "                       batch_size=100, \n",
    "                       corrupt_side='s,o', # corrupt both subject and object\n",
    "                       use_filter={'train':dataset['train'], # Filter to be used for evaluation\n",
    "                                   'valid':dataset['valid'],\n",
    "                                   'test':dataset['test']}\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用实体子集进行破坏评估\n",
    "根据所选择的评估策略，指定用于破坏的实体子集，可以是主语、宾语或两者。请注意，尽管这不是标准的评估协议，但使用实体子集可以生成更有意义的破坏，并且还可以大大减少由于在知识图中的所有实体中抽样破坏而导致的计算开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get all the month present in training\n",
    "months = set(dataset['train'][\n",
    "    dataset['train'][:, 1] == \n",
    "        '/travel/travel_destination/climate./travel/travel_destination_monthly_climate/month'][:, 2])\n",
    "len(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider we are evaluating the below test set which is specific to one predicate\n",
    "# This predicate tells the best time of the year(o) to visit a destination (s)\n",
    "dest_month_test_triples = dataset['test'][\n",
    "    dataset['test'][:, 1] ==\n",
    "        '/travel/travel_destination/climate./travel/travel_destination_monthly_climate/month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 1s/step\n",
      "MR: 1.0833333333333333\n",
      "MRR: 0.9861111111111112\n",
      "hits@1: 0.9833333333333333\n",
      "hits@10: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Let's say we want to evaluate this test set by corrupting the object with only months.\n",
    "# we can pass the months as entities_subset and generate corruptions only using this subset \n",
    "# instead of all entities in the graph\n",
    "# This approach is very useful when the graph size is big and/or \n",
    "# when our hypothesis belongs to a specific predicate type\n",
    "# When graph size is big we can randomly sample fixed number of small subset of entities and use it as corruption\n",
    "\n",
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dest_month_test_triples, \n",
    "                       batch_size=100, \n",
    "                       corrupt_side='o', # corrupt only the object\n",
    "                       entities_subset=months,\n",
    "                       use_filter={'train':dataset['train'], # Filter to be used for evaluation\n",
    "                                   'valid':dataset['valid'],\n",
    "                                   'test':dataset['test']}\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.utils import create_tensorboard_visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, \n",
    "                                  entities_subset=['/m/027rn', '/m/06cx9', '/m/017dcd', '/m/06v8s0', '/m/07s9rl0'], \n",
    "                                  labels=['ent1', 'ent2', 'ent3', 'ent4', 'ent5'],\n",
    "                                  loc = './selected_subset_embeddings_vis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, \n",
    "                                  entities_subset='all',\n",
    "                                  loc = './full_embeddings_vis')\n",
    "\n",
    "# the embeddings can be visualised uncommenting the following command:\n",
    "# ! tensorboard --logdir='./full_embeddings_vis' --port=8891 \n",
    "# open the browser and go to the following URL: http://127.0.0.1:8891/#projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提前停止\n",
    "以下示例展示了如何在训练模型时使用提前停止以及如何创建检查点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ampligraph\n",
    "# Benchmark datasets are under ampligraph.datasets module\n",
    "from ampligraph.datasets import load_fb15k_237\n",
    "# load fb15k-237 dataset\n",
    "dataset = load_fb15k_237()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/29 [==============================] - 3s 94ms/step - loss: 6622.9019\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 6493.6064"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:22.984591: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 262ms/step\n",
      "29/29 [==============================] - 2s 81ms/step - loss: 6493.6064 - val_mrr: 0.0684 - val_mr: 3258.6080 - val_hits@1: 0.0000e+00 - val_hits@10: 0.1790 - val_hits@100: 0.3466\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 1s 34ms/step - loss: 6365.1802\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 6234.0171"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:26.191442: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 219ms/step\n",
      "29/29 [==============================] - 2s 69ms/step - loss: 6234.0171 - val_mrr: 0.0809 - val_mr: 1924.0540 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2102 - val_hits@100: 0.4006\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 2s 59ms/step - loss: 6101.5239\n",
      "Epoch 6/100\n",
      "26/29 [=========================>....] - ETA: 0s - loss: 6003.9404"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:29.466724: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 319ms/step\n",
      "29/29 [==============================] - 2s 55ms/step - loss: 5966.6636 - val_mrr: 0.0777 - val_mr: 1346.6420 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2188 - val_hits@100: 0.4347\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 1s 27ms/step - loss: 5829.4419\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 5690.5356"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:32.519336: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 215ms/step\n",
      "29/29 [==============================] - 2s 72ms/step - loss: 5690.5356 - val_mrr: 0.0793 - val_mr: 1060.0114 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2216 - val_hits@100: 0.4659\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 2s 56ms/step - loss: 5553.0103\n",
      "Epoch 10/100\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 5434.2549"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:35.457800: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 235ms/step\n",
      "29/29 [==============================] - 1s 46ms/step - loss: 5418.0444 - val_mrr: 0.0837 - val_mr: 901.1193 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2216 - val_hits@100: 0.4830\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 1s 26ms/step - loss: 5286.5752\n",
      "Epoch 12/100\n",
      "      2/Unknown - 0s 159ms/step===>..] - ETA: 0s - loss: 5171.8193"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:39.521803: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 147ms/step\n",
      "29/29 [==============================] - 3s 102ms/step - loss: 5158.7935 - val_mrr: 0.0832 - val_mr: 841.9688 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2216 - val_hits@100: 0.4858\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 5035.1396\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 107ms/steposs: 4926.96\n",
      "29/29 [==============================] - 1s 24ms/step - loss: 4916.2104 - val_mrr: 0.0839 - val_mr: 815.3011 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2330 - val_hits@100: 0.4972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:40.702638: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "29/29 [==============================] - 1s 26ms/step - loss: 4801.8057\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 4691.9980"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:42.851130: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 249ms/step\n",
      "29/29 [==============================] - 2s 61ms/step - loss: 4691.9980 - val_mrr: 0.0870 - val_mr: 777.9716 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2301 - val_hits@100: 0.5114\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 1s 20ms/step - loss: 4587.6689\n",
      "Epoch 18/100\n",
      "27/29 [==========================>...] - ETA: 0s - loss: 4499.3130"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:45.612853: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 218ms/step\n",
      "29/29 [==============================] - 2s 69ms/step - loss: 4488.2705 - val_mrr: 0.0833 - val_mr: 755.6392 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2216 - val_hits@100: 0.5199\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 1s 29ms/step - loss: 4393.5967\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 4303.0571"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:47.841319: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 234ms/step\n",
      "29/29 [==============================] - 1s 48ms/step - loss: 4303.0571 - val_mrr: 0.0827 - val_mr: 739.2500 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2216 - val_hits@100: 0.5227\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 4217.0073\n",
      "Epoch 22/100\n",
      "      2/Unknown - 0s 152ms/step===>..] - ETA: 0s - loss: 4140.2354"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:21:50.470017: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 165ms/step\n",
      "29/29 [==============================] - 2s 64ms/step - loss: 4134.3545 - val_mrr: 0.0839 - val_mr: 737.0227 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2244 - val_hits@100: 0.5170\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 22: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28a6d6da0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the KGE model\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
    "\n",
    "# create the model with transe scoring function\n",
    "model = ScoringBasedEmbeddingModel(eta=1, \n",
    "                                     k=10,\n",
    "                                     scoring_type='TransE')\n",
    "\n",
    "\n",
    "# compile the model with loss and optimizer\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "# Use this for checkpoints at regular intervals\n",
    "#checkpoint = tf.keras.callbacks.ModelCheckpoint('./chkpt_transe', monitor='val_mrr', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Use this for early stopping\n",
    "checkpoint = tf.keras.callbacks.EarlyStopping(monitor=\"val_mrr\",            # which metrics to monitor\n",
    "                                              patience=3,                   # If the monitored metric doesnt improve \n",
    "                                                                            # for these many checks the model early stops\n",
    "                                              verbose=1,                    # verbosity\n",
    "                                              mode=\"max\",                   # how to compare the monitored metrics. \n",
    "                                                                            # max - means higher is better\n",
    "                                              restore_best_weights=True)    # restore the weights with best value\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "model.fit(dataset['train'],\n",
    "          batch_size=10000,\n",
    "          epochs=100,\n",
    "          validation_freq=2,                            # Epochs to elapse before next evaluation\n",
    "          validation_batch_size=100,                    \n",
    "          validation_burn_in=1,                         # Epoch to start the validation process\n",
    "          validation_data = dataset['valid'][::100],\n",
    "          callbacks=[checkpoint])                       # Pass the callback to the fit function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 616s 3s/step\n",
      "MR: 600.0142626480086\n",
      "MRR: 0.18888812773966743\n",
      "hits@1: 0.1267491926803014\n",
      "hits@10: 0.3130687934240141\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dataset['test'], # test set\n",
    "                       batch_size=100, # evaluation batch size\n",
    "                       corrupt_side='s,o', \n",
    "                       use_filter={'train':dataset['train'], # Filter to be used for evaluation\n",
    "                                   'valid':dataset['valid'],\n",
    "                                   'test':dataset['test']}\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using ModelCheckpoint then we can restore the checkpoints using restore model\n",
    "# from ampligraph.utils import restore_model\n",
    "# model = restore_model('chkpt_transe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ampligraph\n",
    "# Benchmark datasets are under ampligraph.datasets module\n",
    "from ampligraph.datasets import load_fb15k_237\n",
    "# load fb15k-237 dataset\n",
    "dataset = load_fb15k_237()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 3s 101ms/step - loss: 6625.2280\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 1s 418ms/steposs: 6505.03\n",
      "29/29 [==============================] - 3s 92ms/step - loss: 6505.0376 - val_mrr: 0.0576 - val_mr: 3106.6989 - val_hits@1: 0.0000e+00 - val_hits@10: 0.1562 - val_hits@100: 0.3352\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 49ms/step - loss: 6386.7886\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 101ms/steposs: 6267.646\n",
      "29/29 [==============================] - 2s 63ms/step - loss: 6267.6465 - val_mrr: 0.0673 - val_mr: 2225.0398 - val_hits@1: 0.0000e+00 - val_hits@10: 0.1903 - val_hits@100: 0.3835\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 47ms/step - loss: 6149.4019\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 118ms/steposs: 6029.96\n",
      "29/29 [==============================] - 2s 63ms/step - loss: 6029.9697 - val_mrr: 0.0765 - val_mr: 1584.9233 - val_hits@1: 0.0000e+00 - val_hits@10: 0.1989 - val_hits@100: 0.4119\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 51ms/step - loss: 5910.1436\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 126ms/steposs: 5814.78\n",
      "29/29 [==============================] - 2s 59ms/step - loss: 5789.8135 - val_mrr: 0.0732 - val_mr: 1316.3835 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2045 - val_hits@100: 0.4489\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 43ms/step - loss: 5670.5464\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 1s 186ms/steposs: 5569.24\n",
      "29/29 [==============================] - 2s 70ms/step - loss: 5552.7808 - val_mrr: 0.0728 - val_mr: 1146.0284 - val_hits@1: 0.0000e+00 - val_hits@10: 0.2131 - val_hits@100: 0.4801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2931fd2a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the KGE model\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
    "\n",
    "# create the model with transe scoring function\n",
    "model = ScoringBasedEmbeddingModel(eta=1, \n",
    "                                     k=10,\n",
    "                                     scoring_type='TransE')\n",
    "\n",
    "\n",
    "# compile the model with loss and optimizer\n",
    "model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "# Use this for checkpoints at regular intervals\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('./chkpt1_transe', monitor='val_mrr', verbose=0, \n",
    "                                                save_best_only=True, mode='min')\n",
    "\n",
    "dataset = load_fb15k_237()\n",
    "\n",
    "model.fit(dataset['train'],\n",
    "          batch_size=10000,\n",
    "          epochs=10,\n",
    "          validation_freq=2,\n",
    "          validation_batch_size=100,\n",
    "          validation_burn_in=2,\n",
    "          validation_data = dataset['valid'][::100],\n",
    "          callbacks=[checkpoint])     # Pass the callback to the fit function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 407s 2s/step\n",
      "MR: 921.3923084450533\n",
      "MRR: 0.17730360323410402\n",
      "hits@1: 0.12151384675604267\n",
      "hits@10: 0.2851551032390645\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dataset['test'], # test set\n",
    "                       batch_size=100, # evaluation batch size\n",
    "                       corrupt_side='s,o', \n",
    "                       use_filter={'train':dataset['train'], # Filter to be used for evaluation\n",
    "                                   'valid':dataset['valid'],\n",
    "                                   'test':dataset['test']}\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.utils import save_model\n",
    "# explictly save the model\n",
    "save_model(model, 'saved_model_transE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model does not include a db file. Skipping.\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.utils import restore_model\n",
    "\n",
    "# restore saved models or checkpoints\n",
    "model = restore_model('saved_model_transE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 254s 1s/step\n",
      "MR: 921.3923084450533\n",
      "MRR: 0.17730360323410402\n",
      "hits@1: 0.12151384675604267\n",
      "hits@10: 0.2851551032390645\n"
     ]
    }
   ],
   "source": [
    "# evaluate on the test set\n",
    "ranks = model.evaluate(dataset['test'], # test set\n",
    "                       batch_size=100, # evaluation batch size\n",
    "                       corrupt_side='s,o', \n",
    "                       use_filter={'train':dataset['train'], # Filter to be used for evaluation\n",
    "                                   'valid':dataset['valid'],\n",
    "                                   'test':dataset['test']}\n",
    "                       )\n",
    "\n",
    "# import the evaluation metrics\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "\n",
    "print('MR:', mr_score(ranks))\n",
    "print('MRR:', mrr_score(ranks))\n",
    "print('hits@1:', hits_at_n_score(ranks, 1))\n",
    "print('hits@10:', hits_at_n_score(ranks, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分区训练\n",
    "以下示例展示了如何使用分区训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import numpy as np\n",
    "import ampligraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and predict scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_split: memory before: 848.0Bytes, after: 4.3447MB, consumed: 4.3439MB; exec time: 29.242s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alberto.bernardi/miniforge3/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "2023-02-08 16:47:49.873938: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - 3s 257ms/step - loss: 8690.4297\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 8642.6641\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 8588.0459\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 8524.0557\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 8454.0918\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 2s 167ms/step - loss: 8382.4600\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 8313.3584\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 8245.1660\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 2s 160ms/step - loss: 8180.5093\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 8118.5229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28f31d630>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the KGE model\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
    "\n",
    "PATH_TO_DATASET = 'your/path/to/dataset/'\n",
    "\n",
    "# create the model with transe scoring function\n",
    "partitioned_model = ScoringBasedEmbeddingModel(eta=2, \n",
    "                                               k=50, \n",
    "                                               scoring_type='TransE')\n",
    "partitioned_model.compile(optimizer='adam', loss='multiclass_nll')\n",
    "\n",
    "# Here we have specified the path of the input file\n",
    "# you can also load using default dataloaders load_fb15k_237() and pass numpy array inputs\n",
    "partitioned_model.fit(PATH_TO_DATASET + 'wn18RR/train.txt',\n",
    "                      batch_size=10000, \n",
    "                      partitioning_k=3, # set flag to partition the inputs\n",
    "                      epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "9/9 [==============================] - 14s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20079.140731874144, 0.011132840015629617, 0.0, 0.03625170998632011, 2924)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unfiltered evaluation\n",
    "ranks = partitioned_model.evaluate(PATH_TO_DATASET + 'wn18RR/test.txt', \n",
    "                                   batch_size=400)\n",
    "\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "9/9 [==============================] - 68s 8s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20066.594562243503,\n",
       " 0.01583735697421522,\n",
       " 0.005471956224350205,\n",
       " 0.038132694938440494,\n",
       " 2924)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtered evaluation\n",
    "ranks = partitioned_model.evaluate(PATH_TO_DATASET + 'wn18RR/test.txt', \n",
    "                        batch_size=400,\n",
    "                        corrupt_side='s,o',\n",
    "                        use_filter={'train': PATH_TO_DATASET + 'wn18RR/train.txt',\n",
    "                                    'valid': PATH_TO_DATASET + 'wn18RR/valid.txt',\n",
    "                                    'test': PATH_TO_DATASET + 'wn18RR/test.txt'})\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path ./partitioned_model already exists. This save operation will overwrite the model                 at the specified path.\n",
      "WARNING - Found untraced functions such as _get_ranks while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.utils import save_model\n",
    "save_model(model=partitioned_model, model_name_path='./partitioned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model does not include a db file. Skipping.\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.utils import restore_model\n",
    "model = restore_model('./partitioned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "9/9 [==============================] - 15s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20079.140731874144, 0.011132840015629617, 0.0, 0.03625170998632011, 2924)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unfiltered evaluation\n",
    "ranks = model.evaluate(PATH_TO_DATASET + 'wn18RR/test.txt',\n",
    "                       batch_size=400)\n",
    "\n",
    "from ampligraph.evaluation.metrics import mrr_score, hits_at_n_score, mr_score\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "\n",
      "210 triples containing invalid keys skipped!\n",
      "9/9 [==============================] - 73s 8s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20066.594562243503,\n",
       " 0.01583735697421522,\n",
       " 0.005471956224350205,\n",
       " 0.038132694938440494,\n",
       " 2924)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = model.evaluate(PATH_TO_DATASET + 'wn18RR/test.txt', \n",
    "                        batch_size=400,\n",
    "                        corrupt_side='s,o',\n",
    "                        use_filter={'train': PATH_TO_DATASET + 'wn18RR/train.txt',\n",
    "                                    'valid': PATH_TO_DATASET + 'wn18RR/valid.txt',\n",
    "                                    'test': PATH_TO_DATASET + 'wn18RR/test.txt'})\n",
    "\n",
    "mr_score(ranks), mrr_score(ranks), hits_at_n_score(ranks, 1), hits_at_n_score(ranks, 10), len(ranks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e69f3670cdad0193847aaa0b77be56c05c951fcbdd384ff882dde0464f4de76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
