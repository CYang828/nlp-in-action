{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rl0y0i2miQ-g"
   },
   "source": [
    " # 全网最详细自然语言处理(NLP)流程(Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "了解一个主题，我觉得最好的方法就是先从宏观上去了解做这件事的主要流程是什么，每个流程在做什么，有哪些需要考虑的。今天这篇文章，结合我的工作经验，聊一聊自然语言处理任务的全流程，也聊聊在这些过程中我们都要做什么，背后为什么要这么做，是怎么思考的。\n",
    "\n",
    "本篇文章主要解决：\n",
    "1. NLP 的流程\n",
    "2. 流程中每个环节主要做的事情\n",
    "3. 每一步为什么要这么做，怎么思考的\n",
    "\n",
    "**这里有几点需要提前说明，以防止我们会陷入到一些固化的思维中：**\n",
    "1. NLP 的处理流程不总是线性的\n",
    "2. 经常在处理过程中会有循环\n",
    "3. 这些任务都要具体的根据特定得任务来思考设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK！那我们就开始进入 NLP 任务的 Pipeline 中，通常一个完整的 Pipeline 是下面这样的:\n",
    "\n",
    "![](http://aimaksen.rarelimiting.com/nlp-pipeline.png)\n",
    "- 问题定义\n",
    "- 数据获取\n",
    "- 数据探索\n",
    "- 数据清理和预处理\n",
    "- 分割数据集\n",
    "- 特征工程\n",
    "- 建模\n",
    "- 评估\n",
    "- 推理\n",
    "- 部署"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题定义 (Problem Define)\n",
    "\n",
    "通常在工作中，当我们识别到或者接到一个需求的时候，这个问题往往是一个非常具体的问题，并且有可能不是一个最小化的问题。比如说，你的老板可能会和你说，我们接下来要做一个业务，这个业务呢能够自动的识别出学生作业里面的错别字、语法错误等，并且呢也能给出一些对于这个作业的简要的评价，甚至说能不能给它再打个分数。\n",
    "这是一个非常具体的问题，使我们的用户真实的需求的描述，但是，只有这个并不能帮助我们去做算法，做 NLP 的任务。所以，我们需要把这个业务问题转化成一个算法问题，或者几个算法问题，这些算法或者服务相互的配合，能够帮助我们解决用户的需求。\n",
    "那么，这个把业务转化为几个算法，每个算法又都是什么样的形式，就是我们需要确定的。只有确定了问题，我们才能更好的去用各种各样的算法工具解决这些问题。\n",
    "\n",
    "> 好的问题比回答更重要。\n",
    "\n",
    "这句话在很多场景中都非常的重要，在算法领域中也尤其的重要，如果这一步就走错了，那么后面所做的众多努力，都会白费。所以，我们需要谨慎的对待这个过程，甚至在必要的情况下，我们敢于在后面的环节中提出对问题的置疑，以防止浪费更多的财力物力投入到一个错误的方向。\n",
    "之前我的导师和我说过，等到了一定的阶段，提出问题的能力会成为至关重要的能力，他能决定一个人在专业领域力成就的高度。并且还推荐了锻炼提出问题方面的书，这里也分享给大家，希望大家也能对这个平时我们可能不太关注的能力引起关注，并且训练提高。\n",
    "\n",
    "\n",
    "![question1.jpg](https://aimaksen.rarelimiting.com/question1.jpg)\n",
    "\n",
    "如果有做学术论文选题的也可以看看[这一篇文章](https://pit.ifeng.com/a/20160627/49249371_0.shtml)，是北大的一位法学教授讲如何选题的。\n",
    "\n",
    "能够比较准确的定义问题，我们就能够大概知道我们的这个大的问题，需要通过解决几个小问题解决，并且我们也能够知道每个小问题的范式是什么样的，它所需要的数据是哪些。有了这样的前提，接下来我们就可以来聊一聊，如何获取数据了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXzi_tJTiQ-l"
   },
   "source": [
    "## 数据获取 (Data Acquisition)\n",
    "\n",
    "数据获取工作可以说是在整个 Machine Learning 的过程中，最为核心的工作。但是这件事是个比较艺术的活，它并没有一个完美的解，我们接下来就来聊聊通常情况下，我们该怎么获取数据，又为什么它是个比较艺术的活。\n",
    "\n",
    "![data-acquire.jpg](http://aimaksen.rarelimiting.com/data-acquire.jpg)\n",
    "\n",
    "- 我们可以从图中看到，首先我们要判断，当前是否有不够的数据，如果有充足的数据，那我们就可以直接进行机器学习的建模过程。\n",
    "- 如果没有充足的数据，则我们要向右走，看看是否有什么额外的数据。但是这里大家要注意，到底多少数据才叫充足，这个是没有一个确定的值的，并且在现实的工作中，往往我们并不具备获取充足数据的条件，比如说工期的限制、资源的限制、基础架构的限制等等。所以，在这里我们一定要建立一种意识，就是我们的数据要和算法模型协同的往前走，而不是追求一步就到最终点。\n",
    "- 接下来我们再来看看额外数据。额外数据往往是我们通过其他的渠道获取到的数据，由于自然语言数据相对来说是比较好获得的数据，我们可以从其他的网站上去进行抓取，补充进我们自己的数据中。但是，这里我们一定要选择好额外的数据，这些数据要尽可能的接近于我们想要解决任务的分布，或者说要接近我们“目标用户”所产生的的数据。这也是需要一些方法的，在之后的内容中我们可以再来详细的聊聊怎么来分析文本数据的分布情况。\n",
    "- 我们再来向右看，当我们没有额外数据的补充，或者当我们补充了额外数据之后依然觉得不够，接下来我们就要考虑，使用一些生成的方法或者增强的方法，来生产新的数据了，这个我们也在后面详细的说。\n",
    "\n",
    "在上面我们说使用额外数据的时候，有一种经验的方法是我们经常使用的，就是如果我们所做的任务，之前有相关的学术的数据集的话，那我们就可以快速的使用学术数据来快速的验证我们的算法。这个过程可以大大的加快我们从需求产生到业务落地的速度，但是比较遗憾的是，目前学术界的数据实在太少，能和我们场景匹配的可能性相对比较少。下面也给大家提供一些寻找这些数据集的地方：\n",
    "- [Google Dataset Search](https://datasetsearch.research.google.com/) 谷歌出品的数据集搜索引擎\n",
    "- [Paper with Code](https://paperswithcode.com/)一个 benchmark 和对应代码寻找的地方，除了找数据集也可以找论文和代码看\n",
    "- [Kaggle](https://www.kaggle.com/) 比赛平台，上面有很多公司和网友贡献的数据集\n",
    "- [Open Data on AWS](https://aws.amazon.com/cn/opendata) 亚马逊的开放数据平台\n",
    "\n",
    "这些资源里包含了学术数据集、竞赛数据集、还有我们之前提到的自己去网络上获取的原始数据，这些数据虽然都是属于补充的数据，但是它们之间也存在着一些差异性，我把它们放到下面的表格里，方便进行对比：\n",
    "\n",
    "![data-acquire.jpg](http://aimaksen.rarelimiting.com/compare-dataset.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZFcHw3RiQ-m"
   },
   "source": [
    "在真实的业务里，绝大部分情况都是不理想的情况，我们需要在不理性的情况里，尽可能的寻找方法找到更多的可能性，这往往是工作中最重要的部分。这里暂且放一个小列表，以后再详细的聊聊我遇到过的处理方法。\n",
    "\n",
    "### 不理想的情况的处理\n",
    "\n",
    "- 带有有限注释/标签的初始数据集\n",
    "- 基于正则表达式或启发式标记的初始数据集\n",
    "- 公共数据集 (cf. [Google Dataset Search](https://datasetsearch.research.google.com/) or [kaggle](https://www.kaggle.com/))\n",
    "- 不完整的数据\n",
    "- 产品上的干预\n",
    "- 数据增强\n",
    "\n",
    "### 数据增强\n",
    "\n",
    "- 这是一种利用语言相似性来生产新数据的技术。\n",
    "- 常见的策略包括:\n",
    "    - 同义词替换 (synonym replacement)\n",
    "    - 相关词替换 (based on association metrics)\n",
    "    - 回译 (Back translation)\n",
    "    - 替换实体 (Replacing entities)\n",
    "    - 增加噪音 (e.g. spelling errors, random words)\n",
    "    \n",
    "### 数据标注\n",
    "\n",
    "- 当我们实在没有办法获取到更多的数据，我们只能使用成本最高的方案 - 数据标注。\n",
    "- 数据标注是一个很复杂的工程，由于目前大部分都是机器和人的共同协作来完成，所以这是一个想要做好很难的事儿，具体的也可以单拉一篇说\n",
    "- 这里暂且先放个图\n",
    "\n",
    "![data-labeling.jpg](http://aimaksen.rarelimiting.com/data-labeling.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据探索 EDA(Explore Data Analysis)\n",
    "\n",
    "有了充足的数据，接下来我们要对数据进行探索和分析。在这个过程中，我们要对已有的数据有更加科学的了解。这个部分我们主要会做两件事情\n",
    "\n",
    "1. 使用统计学方法，统计数据中的统计指标\n",
    "2. 使用可视化方法，绘制各种数据观测图\n",
    "\n",
    "两种方法，都是在让我们对数据有着更加准确的认识，以方便我们后续在建模的过程中，能够选择更加合适的模型，已经当我们碰到一些问题的时候，能够更快速的做出准确的判断。\n",
    "\n",
    "这部分也是有着非常多的方法，并且由于每个人对数字和数据的认知方式有比较大的差异性，所以就导致每个人采用的方法也不尽相同。后面，我会介绍一些比较通用的、还有我比较喜欢的方式。\n",
    "\n",
    "还有就是，对于数据的认知实不段完善的过程，并不是说我要在 EDA 的过程里，一下子把所有的可能性都探索穷尽，这样可能会比较影响效率。可能更好的方式是一个螺旋上升式的认识过程，通过 (EDA -> 建模 -> bad case 分析 -> EDA) 这样的流程可能是更理想的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xdlt9U-iQ-n"
   },
   "source": [
    "## 文本抽取和清理 (Text Extraction and Cleanup)\n",
    "\n",
    "在对文本做完 EDA 后，我们会对数据有了一个基础的认识，在对数据认识的过程中，最引起我们关注的就是，文本可能并不像我们想象中的干净，比如其中可能会有各种标签、代码、符号，这些内容在我们后续对自然语言建模过程中，并不是很有意义。所以，我们首先就是要把它们剔除出去，或者也可以叫做抽取干净的文本，都是类似的意思。\n",
    "\n",
    "下面有一些常用的文本抽取和清洗的方法，更多的内容还是放到后以后单独的聊。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Diq2nst3iQ-n"
   },
   "source": [
    "### 文本抽取 (Text Extraction)\n",
    "\n",
    "- 从原始文本中抽取数据\n",
    "    - HTML\n",
    "    - PDF\n",
    "- 相关 vs. 非相关信息\n",
    "    - 非语义信息 (non-textual information)\n",
    "    - 标签 (markup)\n",
    "    - 元数据 (metadata)\n",
    "- 编码格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlnuEXD_iQ-o"
   },
   "source": [
    "#### 从网页中提取文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gewVKa5wiQ-o",
    "outputId": "594dbc59-a1ca-4aa5-aed8-3d1784ed18ae"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://news.google.com/topstories?hl=zh-CN&gl=CN&ceid=CN:zh-Hans\"\n",
    "r = requests.get(url)\n",
    "web_content = r.text\n",
    "soup = BeautifulSoup(web_content, \"html.parser\")\n",
    "title = soup.find_all(\"a\", class_=\"DY5T1d\")\n",
    "first_art_link = title[1][\"href\"].replace(\".\", \"https://news.google.com\", 1)\n",
    "\n",
    "print(first_art_link)\n",
    "art_request = requests.get(first_art_link)\n",
    "art_request.encoding = \"utf8\"\n",
    "soup_art = BeautifulSoup(art_request.text, \"html.parser\")\n",
    "\n",
    "art_content = soup_art.find_all(\"p\")\n",
    "art_texts = [p.text for p in art_content]\n",
    "for text in art_texts:\n",
    "    print(text.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rD9mogK3iQ-q"
   },
   "source": [
    "#### 从扫描的 PDF 中提取文本\n",
    "\n",
    "需要安装 OCR 提取工具 tesseract，安装教程见 https://nanonets.com/blog/ocr-with-tesseract/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypZR_UF9iQ-q",
    "outputId": "9cf587e6-aefa-4449-a888-12f79339864c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pytesseract import image_to_string\n",
    "\n",
    "YOUR_DEMO_DATA_PATH = \"data/\"  # please change your file path\n",
    "filename = YOUR_DEMO_DATA_PATH + \"pdf-firth-text.png\"\n",
    "text = image_to_string(Image.open(filename))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdKv1R92iQ-q"
   },
   "source": [
    "#### Unicode 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nE0X28p_iQ-q",
    "outputId": "5505f6c9-ae58-406f-f868-813fab0cd053"
   },
   "outputs": [],
   "source": [
    "text = \"I feel really 😡. GOGOGO!! 💪💪💪  🤣🤣 ȀÆĎǦƓ\"\n",
    "print(text)\n",
    "text2 = text.encode(\"utf-8\")  # encode the strings in bytes\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvXiND8KiQ-r",
    "outputId": "40c22da5-a6f1-4631-c3f5-6e77b37dda2f"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQyvzlT1iQ-t"
   },
   "source": [
    "- 详细请查阅 [unicodedata documentation](https://docs.python.org/3/library/unicodedata.html) \n",
    "- 其他有用的库\n",
    "    - 拼写检查 (Spelling check): pyenchant, Microsoft REST API\n",
    "    - PDF:  PyPDF, PDFMiner\n",
    "    - OCR: pytesseract\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddFBa9whiQ-t"
   },
   "source": [
    "### 文本预处理 (Text Preprocessing)\n",
    "\n",
    "有了干净的文本，接下来我们要着手把这些文本转换为能够输入到计算机中的数字化表示，为了能够做到这一步，我们要把文本进行一些预处理，方便进行数字化的文本表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ce_hYOBiQ-t"
   },
   "source": [
    "#### 分段和分词 (Segmentation and Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0_4-fYhiQ-t",
    "outputId": "1806db13-61d2-4a42-8f89-72e22c9cfd05"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"\"\"\n",
    "Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
    "\"\"\"\n",
    "\n",
    "## sent segmentation\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "## word tokenization\n",
    "for sent in sents:\n",
    "    print(sent)\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxX_eM6DiQ-u"
   },
   "source": [
    "- 经常使用的预处理 （preprocessing）\n",
    "    - 停用词 (Stopword) 移除\n",
    "    - Stemming 和lemmatization\n",
    "    - 数字或标点移除\n",
    "    - 大小写标准化\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVrAuhfJiQ-u"
   },
   "source": [
    "#### 删除停用词、标点符号和数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SD1S8lsZiQ-u",
    "outputId": "962624f2-e21b-4ef5-decb-3c3892de766d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "text = \"Mr. John O'Neil works at Wonderland, located at 245 Goleta Avenue, CA., 74208.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)\n",
    "\n",
    "# remove stopwords, punctuations, digits\n",
    "for w in words:\n",
    "    if w not in eng_stopwords and w not in punctuation and not w.isdigit():\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8BRzw57iQ-u"
   },
   "source": [
    "#### Stemming 和 lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GoRYlcUiQ-u",
    "outputId": "98d2a94b-60ce-4b65-e877-db22b1cfe195"
   },
   "outputs": [],
   "source": [
    "## Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"cars\", \"revolution\", \"better\"]\n",
    "print([stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiwUYRdTiQ-v",
    "outputId": "e2c09901-82ba-46df-f462-806f901e4003"
   },
   "outputs": [],
   "source": [
    "## Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "## Wordnet requires POS of words\n",
    "poss = [\"n\", \"n\", \"a\"]\n",
    "\n",
    "for w, p in zip(words, poss):\n",
    "    print(lemmatizer.lemmatize(w, pos=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oo-GlGYhiQ-v"
   },
   "source": [
    "还有一些预处理的方法，包括以下这些。当然，这些也只是一部分而已，在特定的业务场景里，还会有些特殊的预处理的手段。这些，我们也放到后面说，这里先对预处理有个轮廓型的认识就好。\n",
    "\n",
    "- 和任务相关的预处理 (preprocessing)\n",
    "    - Unicode 标准化\n",
    "    - 语言检测 (Language detection)\n",
    "    - 混合编码 (Code mixing)\n",
    "    - 同音异形 (Transliteration) (e.g., using piyin for Chinese words in English-Chinese code-switching texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbfufLaciQ-v"
   },
   "source": [
    "### 预处理的重要提醒\n",
    "- 预处理技术在业务场景里很多时候都是大规模的，往往很难用单机进行处理，所以为了能够处理好数据，往往需要我们具备一定的大数据数据处理的技术，这个我们也会在后面的环节里说到\n",
    "- 并不是所有的预处理过程都是必要的，要根据具体情况分析，预处理所带来的的好处和弊端\n",
    "- 这些步骤不是顺序的\n",
    "- 这些步骤取决于任务的\n",
    "- 预处理的目标\n",
    "    - 文本标准化 (Text Normalization)\n",
    "    - 文本单词化 (Text Tokenization)\n",
    "    - 文本增补和丰富 (Text Enrichment/Annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMbPXmUOiQ-v"
   },
   "source": [
    "## 特征工程 (Feature Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xob6EYgsiQ-v"
   },
   "source": [
    "### 什么是特征工程 (feature engineering)?\n",
    "\n",
    "- 它是指将提取和预处理的文本输入机器学习算法的过程\n",
    "- 它旨在将文本的特征捕捉到一个数字向量中，该向量可以被ML算法理解。(Cf. *construct*, *operational definitions*, and *measurement* in experimental science)\n",
    "- 简言之，它涉及到如何有意义地定量表示文本, i.e., text representation.\n",
    "\n",
    "特征工程在传统的机器算法和深度学习当中，使用的方法和所处的地位是不一样的。总的来说在传统的机器学习方法中，特城工程有着非常重要的地位，而在深度学习当中，特征工程的地位略有下降，甚至在有些算法中，不再需要手动的进行特诊工程，而是通过神经网络自动的完成。但是，在一些特别的任务中，特征工程依然非常的重要，所以也要引起足够的重视。\n",
    "\n",
    "下面简单的提到一些在 NLP 任务中，使用的特征工程的方法。这里也只是暂对这些由所熟悉即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F15WIiBGiQ-w"
   },
   "source": [
    "### 传统机器学习算法的特征工程 \n",
    "\n",
    "- 基于词的频率表\n",
    "- 文字袋表示法 \n",
    "- 特定于域的词频列表 \n",
    "- 基于领域特定知识的手工特征 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Lr8MoHGiQ-w"
   },
   "source": [
    "### 深度学习的特征工程\n",
    "\n",
    "- Deep Learning 直接将文本作为模型的输入\n",
    "- Deep Learning 模型能够从文本中学习特征 (e.g., embeddings)\n",
    "- 其代价是，该模型往往难以解释\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vr3qBK09iQ-w"
   },
   "source": [
    "## 建模 (Modeling)\n",
    "\n",
    "到了这里，我们应该已经有了一些可以被用来输入的数字化表示的文本数据，接下来，我们就要开始做数据建模的工作了。建模，对于很多刚开始学习机器学习的同学会非常的迷惑，什么是建模呢？\n",
    "\n",
    "其实，建模非常的简单。就有点像我们在小学所做的找规律一样，比如说\n",
    "\n",
    "> 1 3 5 7 9 11 ... 找到符合这些数据规律的函数\n",
    "\n",
    "那现在我们一眼就能看出来，这就是一个基数的序列，我们使用函数的形式表示就是 y = 2x + 1 (x>=0)。\n",
    "\n",
    "这个过程其实就是建模的过程，我们通过大脑，把一个序列建模成了 y 这个函数。那在机器学习的任务中，就是我们要找到在我们现有的数据中，所具有的函数表示。当然，由于我们的数据非常多，非常的复杂，所以我们很难像上面那个例子一样，寻找到一个能够 100% 拟合的函数。而是寻找到一个绝大多数符合的函数，越多数据符合函数，那就说明函数拟合的越好。还有一点需要注意的是，我们需要到的这个函数，由于数据的复杂性，所以它的表现形式也是非常的复杂和多元化。后面，我们会讲解很多种模型用来拟合不同的数据模式，大致上从简单到复杂是以下这样："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mi7Z_24AiQ-w"
   },
   "source": [
    "### 从简单到复杂\n",
    "\n",
    "- 从启发式或规则开始\n",
    "- 不同 ML 模型的实验\n",
    "    - 从启发式到特征\n",
    "    - 从手动注释到自动提取\n",
    "    - 特征重要性 (Feature importance/weights) \n",
    "- 找到最佳的模型\n",
    "    - Ensemble 和 stacking\n",
    "    - 重做 feature engineering\n",
    "    - 迁移学习 (Transfer learning)\n",
    "    - 重新应用启发式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6j3mLjs6iQ-w"
   },
   "source": [
    "## 评估 (Evaluation)\n",
    "\n",
    "![](http://aimaksen.rarelimiting.com/evaluation1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wL6pEOyMiQ-x"
   },
   "source": [
    "### 为什么要 evaluation?\n",
    "\n",
    "- 我们需要知道我们建立的模型有多好 \n",
    "- 与评估方法相关的因素 \n",
    "    - 建模方法 (Model building)\n",
    "    - 部署 (Deployment)\n",
    "    - 生产 (Production)\n",
    "- ML度量与业务度量 （ML metrics vs. Business metrics）\n",
    "\n",
    "能够准确科学的进行评估并不是一件简单的事情，往往在企业里有很复杂的流程设计，这些我们会在后面细说。整体上，评估的逻辑主要是从内部和外部进行评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YD4RGO1biQ-x"
   },
   "source": [
    "### 内在评价与外在评价\n",
    "\n",
    "- 以垃圾邮件分类系统为例\n",
    "- 内在评价:\n",
    "    - 垃圾邮件分类/预测的精度和召回率\n",
    "- 外在评价:\n",
    "    - 用户在垃圾邮件上花费的时间\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBPVYewLiQ-x"
   },
   "source": [
    "### 一般性原则\n",
    "\n",
    "- 在外部评估之前先进行内部评估。\n",
    "- 外部评估成本更高，因为它通常涉及人工智能团队以外的项目干系人\n",
    "- 只有当我们在内在评价中获得一致的好结果时，我们才应该进行外在评价\n",
    "- 内在的不良结果往往意味着外在的不良结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LArXgxVLiQ-x"
   },
   "source": [
    "### 通用的内在指标 (Intrinsic Metrics)\n",
    "\n",
    "- 评估指标选择原则 \n",
    "- 标签的数据类型 (ground truths)\n",
    "    - 二元 (Binary) (e.g., sentiment)\n",
    "    - 序型 (Ordinal) (e.g., informational retrieval)\n",
    "    - 分类 (Categorical) (e.g., POS tags)\n",
    "    - 文本 (Textual) (e.g., named entity, machine translation, text generation)\n",
    "- 自动与人工评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRtQF5YiiQ-x"
   },
   "source": [
    "## 后建模阶段 (Post-Modeling Phases)\n",
    "\n",
    "后建模阶段主要包括推理和部署，以及部署后的相关监控和运维。\n",
    "\n",
    "- 在生产环境中部署模型 (e.g., web service)\n",
    "- 定期监控系统性能 \n",
    "- 用新的数据更新系统\n",
    "\n",
    "上面这些设计大量的工程中的细节，在后面我们会详细的说：\n",
    "\n",
    "- 分布式部署\n",
    "- 高性能部署\n",
    "- 数据对模型的更新\n",
    "- 监控等等\n",
    "\n",
    "好了，自然语言处理流程就介绍这些，相信读到这里，你已经对自然语言处理的全流程，有了比较宏观的了解了，后面就是在这个框架中，补充针对每一个过程的细节。如果你对这些感兴趣，可以关注我，后面文章见~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d0f4eeb065279ab646599e1fb80dbec7830f541a8c87b319bf23cae632500114"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
