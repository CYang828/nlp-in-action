{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 意图分类模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Word Embeddings\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "print(f'gensim: {gensim.__version__}')\n",
    "\n",
    "# Text\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "# Storing as objects via serialization\n",
    "from tempfile import mkdtemp\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Visualization \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "# Directory\n",
    "import os\n",
    "import yaml\n",
    "import collections\n",
    "import scattertext as st\n",
    "import math\n",
    "\n",
    "# Cool progress bars\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()  # Enable tracking of execution progress\n",
    "\n",
    "## LOADING OBJECTS\n",
    "processed_inbound = pd.read_pickle('objects/processed_inbound_extra.pkl')\n",
    "processed = pd.read_pickle('objects/processed.pkl')\n",
    "\n",
    "# Reading back in intents\n",
    "with open(r'objects/intents.yml') as file:\n",
    "    intents = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "# Previewing\n",
    "print(f'\\nintents:\\n{intents}')\n",
    "print(f'\\nprocessed:\\n{processed.head()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Doc2Vec收集推文\n",
    "\n",
    "我可以使用我的Doc2Vec表示，根据推文的余弦相似性，找到与推文的广义意图版本最相似的前1000条推文。\n",
    "\n",
    "启发式搜索是指一种搜索策略，它试图通过基于给定的启发式函数或成本度量迭代改进解决方案来优化问题。我的成本度量是试图获得最接近的余弦距离。\n",
    "\n",
    "这真的很酷。所以我基本上用我的训练数据训练了我的doc2vec模型，这就是processed_inbound。实际上，我可以根据我的训练数据计算一个向量来向量化这个词。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练我的 Doc2Vec\n",
    "这是为word2vec推广到段落而开发的一种方法。Doc2Vec取它们的平均值，每条推文都表示为一个嵌入，因此您具有一致的维度。\n",
    "\n",
    "Word2Vec使用连续单词袋，在每个单词周围创建一个滑动窗口，从上下文（单词周围）和Skip Gram模型预测它。Doc2Vec正是基于此。\n",
    "\n",
    "* https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "* https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "* https://rare-technologies.com/doc2vec-tutorial/\n",
    "\n",
    "它基本上是单词到向量，但基本上采用标准的单词到向量模型，并添加一个额外的向量来表示段落，称为段落向量。输入单词序列，然后他们用它们来预测下一个单词，并检查预测是否正确。如果预测是正确的，它会对不同的单词组合进行多次预测。\n",
    "\n",
    "它与word2vec相同，但处于文档级别，而不是单词级别。我下面的实现基于[这里](https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)以及通过滚动Gensim的文档来更精细地了解每一步。\n",
    "\n",
    "\n",
    "## My Intents:\n",
    "\n",
    "<img src=\"visualizations/intent_list.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "更新：\n",
    "*决定删除lost_replace，因为它与修复很难区分，因为大多数丢失东西的客户在技术上也需要解决问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整合\n",
    "基本上，我有两种方法来获取我的意向训练数据（每种1000个）：\n",
    "* **Doc2Vec:** 我将使用doc2vec从理想化的示例中综合生成一些意图示例\n",
    "* **Manual:** 我将通过复制和手动综合生成一些意图示例（如问候语，因为当前数据并不代表这一点）\n",
    "* **Hybrid:** 有些意图是，我将采用混合方法，其中50%可能是我生成的数据，50%可能是"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making my idealized dataset - generating N Tweets similar to this artificial Tweet\n",
    "# This will then be concatenated to current inbound data so it can be included in the doc2vec training\n",
    "\n",
    "# Version 1\n",
    "ideal = {'Greeting': 'hi hello yo hey whats up howdy morning',\n",
    "        'Update': 'have problem with update'}\n",
    "# Version 2 - I realized that keywords might get the job done, and it's less risky to \n",
    "# add more words for the association power because it's doc2vec\n",
    "ideal = {'battery': 'battery power', \n",
    "         'forgot_password': 'password account login',\n",
    "         'payment': 'credit card payment pay',\n",
    "         'update': 'update upgrade',\n",
    "         'info': 'info information'\n",
    "#          ,'lost_replace': 'replace lost gone missing trade'\n",
    "         ,'location': 'nearest apple location store'\n",
    "        }\n",
    "\n",
    "def add_extra(current_tokenized_data, extra_tweets):\n",
    "    ''' Adding extra tweets to current tokenized data'''\n",
    "    \n",
    "    # Storing these extra Tweets in a list to concatenate to the inbound data\n",
    "    extra_tweets = pd.Series(extra_tweets)\n",
    "\n",
    "    # Making string form\n",
    "    print('Converting to string...')\n",
    "    string_processed_data = current_tokenized_data.progress_apply(\" \".join)\n",
    "\n",
    "    # Adding it to the data, updating processed_inbound\n",
    "    string_processed_data = pd.concat([string_processed_data, extra_tweets], axis = 0)\n",
    "\n",
    "    # We want a tokenized version\n",
    "    tknzr = TweetTokenizer(strip_handles = True, reduce_len = True)\n",
    "#     print('Tokenizing...')\n",
    "#     string_processed_data.progress_apply(tknzr.tokenize)\n",
    "    return string_processed_data\n",
    "\n",
    "# Getting the lengthened data\n",
    "processed_inbound_extra = add_extra(processed['Processed Inbound'], list(ideal.values()))\n",
    "\n",
    "# Saving updated processed inbound into a serialized saved file\n",
    "processed_inbound_extra.to_pickle('objects/processed_inbound_extra.pkl')\n",
    "\n",
    "processed_inbound_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_inbound_extra[-7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_inbound_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我先贴标签，然后开始训练我的模特！这就像训练一个神经网络。至于参数，我将每个向量设置为20维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_doc2vec(string_data, max_epochs, vec_size, alpha):\n",
    "    # Tagging each of the data with an ID, and I use the most memory efficient one of just using it's ID\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) \n",
    "                   for i, _d in enumerate(string_data)]\n",
    "    \n",
    "    # Instantiating my model\n",
    "    model = Doc2Vec(size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm =1)\n",
    "\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model.train(tagged_data, total_examples = model.corpus_count, epochs=model.iter)\n",
    "        # Decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # Fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "\n",
    "    # Saving model\n",
    "    model.save(\"models/d2v.model\")\n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "# Training\n",
    "train_doc2vec(processed_inbound_extra, max_epochs = 100, vec_size = 20, alpha = 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in my model\n",
    "model = Doc2Vec.load(\"models/d2v.model\")\n",
    "\n",
    "# Storing my data into a list - this is the data I will cluster\n",
    "inbound_d2v = np.array([model.docvecs[i] for i in range(processed_inbound_extra.shape[0])])\n",
    "\n",
    "# Saving\n",
    "with open('objects/inbound_d2v.pkl', 'wb') as f:\n",
    "    pickle.dump(inbound_d2v, f)\n",
    "\n",
    "inbound_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inbound_d2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以前，我们在矢量器中没有距离的概念，它们实际上没有特定的含义。这是一种更好的方式，因为它可以捕捉单词之间的上下文表示！我的聚类应该比tfidf或单词袋要好得多。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec接受了哪些培训？\n",
    "需要记住的一件事是，找到这种嵌入是在什么上训练的。我们不希望它在学术数据上训练，因为推特与学术论文在一个完全不同的领域。\n",
    "\n",
    "查看gensim[文档](https://radimrehurek.com/gensim/models/doc2vec.html)对于doc2vec，它似乎与word2vec一样被训练，只是现在它们还使用了段落上下文向量。这意味着它很可能是在谷歌新闻上训练的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法\n",
    "\n",
    "最初，为了获得前1000条类似的推文，我尝试使用现有的数据。但我不认为这会产生最准确的结果，因为你捕捉的不是最具代表性的推文。出于这个原因，我自己制作了所有这些具有基本代表性的推文（你可以在上面的“理想”dict中看到这一点。目标是找到一个理想化的、完整的意图表示。然后，我使用我的doc2vec表示，根据余弦相似性找到前1000条最相似的推文。\n",
    "### 软件包探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding and making idealized versions of each intent so that I can find top 1000 to it:\n",
    "intents_ideal = {'app': ['app', 'prob']}\n",
    "inferred_vectors = []\n",
    "\n",
    "for keywords in intents_ideal.values():\n",
    "    inferred_vectors.append(model.infer_vector(keywords))\n",
    "    \n",
    "inferred_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.similarity(inferred_vectors[0], inbound_d2v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'hi hello yo hey whats up'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查找意向标记\n",
    "我想获得我的代表性推文的标签，因为这就是doc2vec的“model.asimilation”方法作为参数来生成与之相似的前N条推文的原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码块不是最有效的代码，而且计算起来很长，但它很有效！它基本上是搜索我所有处理过的入站推文，并查找我的代表推文的标签，如我的输出和“intents_tags”字典中所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Storing my representative tweets and intents in this dictionary\n",
    "# Just need to add to this dictionary and the rest of the code block does the work for you\n",
    "# To find a suitable representative tweet for this: I used the keyword EDA functions in notebook 1.1\n",
    "\n",
    "# Version 1\n",
    "intents_repr = {'Battery': ['io', 'drain', 'battery', 'iphone', 'twice', 'fast', 'io', 'help'],\n",
    "    'Update': ['new', 'update', 'i️', 'make', 'sure', 'download', 'yesterday'],\n",
    "    'iphone': ['instal', 'io', 'make', 'iphone', 'slow', 'work', 'properly', 'help'],\n",
    "    'app': ['app', 'still', 'longer', 'able', 'control', 'lockscreen'],\n",
    "    'mac': ['help','mac','app','store','open','can','not','update','macbook','pro','currently','run','o','x',\n",
    "  'yosemite'], 'greeting': ['hi', 'hello', 'yo', 'hey', 'whats', 'up']\n",
    "    }\n",
    "# You could see that in version 1, I try to use existing tweets, but that isn't really the best strategy and\n",
    "# it doesn't yield the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Version 2\n",
    "tknzr = TweetTokenizer(strip_handles = True, reduce_len = True)\n",
    "## Just tokenizing all the values of ideal' values to be able to be fed in to matching function\n",
    "# intents_repr = dict(zip(ideal.keys(), [tknzr.tokenize(v) for v in ideal.values()]))\n",
    "# Pythonic way\n",
    "intents_repr = {k:tknzr.tokenize(v) for k, v in ideal.items()}\n",
    "print(intents_repr)\n",
    "\n",
    "# Saving intents_repr into YAML\n",
    "with open('objects/intents_repr.yml', 'w') as outfile:\n",
    "    yaml.dump(intents_repr, outfile, default_flow_style=False)\n",
    "\n",
    "# Storing tags in order of the dictionary above\n",
    "tags = []\n",
    "\n",
    "tokenized_processed_inbound = processed_inbound.apply(tknzr.tokenize)\n",
    "# Find the index locations of specific Tweets\n",
    "def report_index_loc(tweet, intent_name):\n",
    "    ''' Takes in the Tweet to find the index for and returns a report of that Tweet index along with what the \n",
    "    representative Tweet looks like'''\n",
    "    try:\n",
    "        tweets = []\n",
    "        for i,j in enumerate(tokenized_processed_inbound):\n",
    "            if j == tweet:\n",
    "                tweets.append((i, True))\n",
    "            else:\n",
    "                tweets.append((i, False))\n",
    "        index = []\n",
    "        get_index = [index.append(i[0]) if i[1] == True else False for i in tweets] # Comprehension saves space\n",
    "\n",
    "        preview = processed_inbound.iloc[index]\n",
    "\n",
    "        # Appending to indexes for dictionary\n",
    "        tags.append(str(index[0]))\n",
    "    except IndexError as e:\n",
    "        print('Index not in list, move on')\n",
    "        return\n",
    "        \n",
    "    return intent_name, str(index[0]), preview\n",
    "\n",
    "# Reporting and storing indexes with the function\n",
    "print('TAGGED INDEXES TO LOOK FOR')\n",
    "for j,i in intents_repr.items():\n",
    "    try:\n",
    "        print('\\n{} \\nIndex: {}\\nPreview: {}'.format(*report_index_loc(i,j)))\n",
    "    except Exception as e:\n",
    "        print('Index ended')\n",
    "\n",
    "# Pythonic way of making new dictionary from 2 lists\n",
    "intents_tags = dict(zip(intents_repr.keys(), tags))\n",
    "intents_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great! Now I can get the training data for my battery intent (as an example)\n",
    "similar_doc = model.docvecs.most_similar('76066',topn = 1000)\n",
    "# Preview\n",
    "similar_doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_doc = model.docvecs.most_similar('76070',topn = 1000)\n",
    "similar_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练数据合成\n",
    "### 1.基于相似性向训练数据添加意图如上所示，右侧元组元素是余弦相似性。\n",
    "\n",
    "我们只是取了前1000个，类似于意图的基本理想化版本（我们主要基于关键字）。\n",
    "\n",
    "### 2.手动添加意图这些意图是用一种更手动的不同方法生成的。\n",
    "\n",
    "我会生成尽可能多的例子，然后我通过复制它来强制它，直到它达到1000个训练例子，以保持类平衡。\n",
    "再一次，以下是我想补充的所有意图：\n",
    "<img src=“visualizations/intent_list.png”alt=“Drawing”style=“width=300px；”/>\n",
    "\n",
    "### 3.加上混合意图，我使用了上一本笔记本中显示的关键字探索，发现更新和修复之间有很多重叠。\n",
    "\n",
    "因此，对于这两种情况，我将使用doc2vec生成一部分，其余部分我将手动插入示例——其想法是平衡过拟合或噪声，并输入正确的信号。\n",
    "\n",
    "_一个特殊的情况可能是超出范围，我可能会找到另一种方法来处理它，因为我无法生成所有这种意图的例子_\n",
    "第4步是将数据转换为长格式，NN可以被馈送到该格式，最后一步是保存它。\n",
    "我从spaCy文档中了解到了非地转遗忘问题，在这个问题中，你不应该迭代相同的值，因为这样做可以有效地改变损失函数，你将创建一个无法很好地泛化的模型。这最终是一个漫长的过程，因为我必须试验什么最有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for stopwords because I don't want to include them in the manually representative intents\n",
    "# This is something that I manually tune to the dataframe (for step 2 of this process)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english').index('to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docvecs.most_similar('10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intents_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示用户更新或损坏。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing how to tokenize numpy array\n",
    "vals = [word_tokenize(tweet) for tweet in list(processed_inbound.iloc[[10,1]].values)]\n",
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting top n tweets similar to the 0th Tweet\n",
    "# This will return the a list of tuples (i,j) where i is the index and j is \n",
    "# the cosine similarity to the tagged document index\n",
    "\n",
    "# Storing all intents in this dataframe\n",
    "train = pd.DataFrame()\n",
    "# intent_indexes = {}\n",
    "\n",
    "# 1. Adding intent content based on similarity\n",
    "def generate_intent(target, itag):\n",
    "    similar_doc = model.docvecs.most_similar(itag,topn = target)\n",
    "    # Getting just the indexes\n",
    "    indexes = [int(i[0]) for i in similar_doc]\n",
    "#     intent_indexes[intent_name] = indexes\n",
    "    # Actually seeing the top 1000 Tweets similar to the 0th Tweet which seems to be about updates\n",
    "    # Adding just the values, not the index\n",
    "    # Tokenizing the output\n",
    "    return [word_tokenize(tweet) for tweet in list(processed_inbound.iloc[indexes].values)]\n",
    "\n",
    "# Updating train data\n",
    "for intent_name, itag in intents_tags.items():\n",
    "    train[intent_name] = generate_intent(1000, itag)\n",
    "\n",
    "# 2. Manually added intents\n",
    "# These are the remainder intents\n",
    "manually_added_intents = {\n",
    "    'speak_representative': [['talk','human','please'],\n",
    "                             ['let','me','talk','to','apple','support'], \n",
    "                             ['can','i','speak','agent','person']], \n",
    "    'greeting': [['hi'],['hello'], ['whats','up'], ['good','morning'],\n",
    "                 ['good','evening'], ['good','night']],\n",
    "    'goodbye': [['goodbye'],['bye'],['thank'],['thanks'], ['done']], \n",
    "    'challenge_robot': [['robot','human'], ['are','you','robot'],\n",
    "                       ['who','are','you']]\n",
    "}\n",
    "\n",
    "# Inserting manually added intents to data\n",
    "\n",
    "def insert_manually(target, prototype):\n",
    "    ''' Taking a prototype tokenized document to repeat until\n",
    "    you get length target'''\n",
    "    factor = math.ceil(target / len(prototype))\n",
    "    content = prototype * factor\n",
    "    return [content[i] for i in range(target)]\n",
    "\n",
    "# Updating training data\n",
    "for intent_name in manually_added_intents.keys():\n",
    "    train[intent_name] = insert_manually(1000, [*manually_added_intents[intent_name]])\n",
    "\n",
    "# 3. Adding in the hybrid intents\n",
    "\n",
    "hybrid_intents = {'update':(300,700,[['want','update'], ['update','not','working'], \n",
    "                                     ['phone','need','update']], \n",
    "                            intents_tags['update']),\n",
    "                  'info': (800,200, [['need','information'], \n",
    "                                       ['want','to','know','about'], ['what','are','macbook','stats'],\n",
    "                                    ['any','info','next','release','?']], \n",
    "                             intents_tags['info']),\n",
    "                  'payment': (300,700, [['payment','not','through'], \n",
    "                                       ['iphone', 'apple', 'pay', 'but', 'not', 'arrive'],\n",
    "                                       ['how','pay','for', 'this'],\n",
    "                                       ['can','i','pay','for','this','first']], \n",
    "                             intents_tags['payment']),\n",
    "                  'forgot_password': (600,400, [['forgot','my','pass'], ['forgot','my','login'\n",
    "                                ,'details'], ['cannot','log','in','password'],['lost','account','recover','password']], \n",
    "                             intents_tags['forgot_password'])\n",
    "                 }\n",
    "\n",
    "def insert_hybrid(manual_target, generated_target, prototype, itag):\n",
    "    return insert_manually(manual_target, prototype) + list(generate_intent(generated_target, itag))\n",
    "\n",
    "# Updating training data\n",
    "for intent_name, args in hybrid_intents.items():\n",
    "    train[intent_name] = insert_hybrid(*args)\n",
    "\n",
    "# 4. Converting to long dataframe from wide that my NN model can read in for the next notebook - and wrangling\n",
    "neat_train = pd.DataFrame(train.T.unstack()).reset_index().iloc[:,1:].rename(columns={'level_1':'Intent', 0: 'Utterance'})\n",
    "# Reordering\n",
    "neat_train = neat_train[['Utterance','Intent']]\n",
    "\n",
    "# 5. Saving this raw training data into a serialized file\n",
    "neat_train.to_pickle('objects/train.pkl')\n",
    "\n",
    "# Styling display\n",
    "show = lambda x: x.head(10).style.set_properties(**{'background-color': 'black',                                                   \n",
    "                                    'color': 'lawngreen',                       \n",
    "                                    'border-color': 'white'})\\\n",
    ".applymap(lambda x: f\"color: {'lawngreen' if isinstance(x,str) else 'red'}\")\\\n",
    ".background_gradient(cmap='Blues')\n",
    "\n",
    "print(train.shape)\n",
    "show(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我对这些很满意。如果你检查一下，它们看起来很有前途！我并不太担心我的预处理器错过的表情符号——它们的频率很少，只会增加噪音。同样的事情也适用于其他事情，比如语言，因为我也看到了一条印尼推特。这可能是一件好事，因为我们不希望我们的模型过度拟合，它甚至可能有助于我的模型的可推广性。\n",
    "\n",
    "最糟糕的结果可能来自“lost_replace”意图，因为正如关键字EDA中所示，无论如何都没有太多这样的内容。我可能会把它取下来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(neat_train.shape)\n",
    "show(neat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neat_train.tail(44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，这些看起来都很有希望，因为它们似乎都与各自的桶有一些关系。一个表情符号从我的预处理功能中逃脱了，但它们的数量并没有少到我觉得现在不需要删除它，它们只是噪音。\n",
    "\n",
    "还要注意，如果你比较这些数据的尾部和头部，“更新”是作为模板和我的推文的混合生成的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "丢失和更换-产品出现问题。我的iphone很热，你能换一下吗？丢了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing the real data for an intent\n",
    "intent_name = 'lost_replace'\n",
    "view = processed.iloc[intent_indexes[intent_name]]['Real Inbound']\n",
    "[*view]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 意向桶评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing word rank table dataframes in this dict\n",
    "wordranks = {}\n",
    "\n",
    "# For visualizing top 10\n",
    "def top10_bagofwords(data, output_name, title):\n",
    "    ''' Taking as input the data and plots the top 10 words based on counts in this text data'''\n",
    "    bagofwords = CountVectorizer()\n",
    "    # Output will be a sparse matrix\n",
    "    inbound = bagofwords.fit_transform(data)\n",
    "    # Inspecting of often contractions and colloquial language is used\n",
    "    word_counts = np.array(np.sum(inbound, axis=0)).reshape((-1,))\n",
    "    words = np.array(bagofwords.get_feature_names())\n",
    "    words_df = pd.DataFrame({\"word\":words, \n",
    "                             \"count\":word_counts})\n",
    "    words_rank = words_df.sort_values(by=\"count\", ascending=False)\n",
    "    wordranks[output_name] = words_rank\n",
    "    # words_rank.to_csv('words_rank.csv') # Storing it in a csv so I can inspect and go through it myself\n",
    "    # Visualizing top 10 words\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.barplot(words_rank['word'][:10], words_rank['count'][:10].astype(str), palette = 'inferno')\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Saving\n",
    "    plt.savefig(f'visualizations/next_ver/{output_name}.png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing my bucket evaluations here - seeing what each distinct bucket intent means\n",
    "for i in train.columns:\n",
    "    top10_bagofwords(train[i].apply(\" \".join), f'bucket_eval/{i}', f'Top 10 Words in {i} Intent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial thoughts:\n",
    "\n",
    "To be honest, I feel like the way I should get my training data for greeting is not the best. There are a lot of words that are similar between buckets. As an example, for mac, it's a little concerning that iphone is the most common word!\n",
    "\n",
    "After changing method (version 2):\n",
    "\n",
    "The words and results make a lot more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Investigating bag of word frequencies at a more granular level\n",
    "wordranks['bucket_eval/mac'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正在为rasa生成文本文件\n",
    "Rasa API要求将这种格式的数据输入到他们的机器人程序中。我在训练中使用自己的训练数据，但这是为了试验他们的工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting NLU.md training data in correct form for Rasa Bot\n",
    "with open('data/train_rasa/train_v3.txt', 'w') as t:\n",
    "    for intent in train.columns:\n",
    "        t.write(f'## intent: {intent}\\n')\n",
    "        for tweet in train[intent]:\n",
    "            t.write('- ' + \" \".join(tweet) + '\\n')\n",
    "        t.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is just a cell to log my progress of how my method was doing at first\n",
    "\n",
    "没有表情符号的类似推文“[新]、“更新”、“我”️', 'make'，'sure'，'下载'，'昨天']`\n",
    "\n",
    "格式为：`（索引标记，余弦相似性）`\n",
    "\n",
    "[('72326', 0.8154675364494324),\n",
    " ('32166', 0.8151031732559204),\n",
    " ('29461', 0.8027088642120361),\n",
    " ('5942', 0.7968393564224243),\n",
    " ('54836', 0.7879305481910706),\n",
    " ('30359', 0.7861931324005127),\n",
    " ('66201', 0.7817540168762207),\n",
    " ('50109', 0.7796376943588257),\n",
    " ('59490', 0.7793254852294922),\n",
    " ('46644', 0.7775745391845703),\n",
    " ('58410', 0.7734568119049072),\n",
    " ('26164', 0.7674931287765503),\n",
    " ('14867', 0.7673683166503906),\n",
    " ('25813', 0.766610860824585),\n",
    " ('47880', 0.7642890214920044),\n",
    " ('30945', 0.76273113489151),\n",
    " ('74155', 0.7582229971885681),\n",
    " ('33346', 0.7577282190322876),\n",
    " ('9502', 0.7569847702980042),\n",
    " ('64871', 0.7567278146743774)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scattertext from the spaCy universe for EDA\n",
    "This [kernel](https://www.kaggle.com/psbots/customer-support-meets-spacy-universehttps://www.kaggle.com/psbots/customer-support-meets-spacy-universe) 向我展示了spaCy的散点文本工具的功能！所以我也想亲自去做，希望能获得有用的见解。\n",
    "\n",
    "正如文档中所说，散点文本是“一种在中小型语料库中找到区别术语的工具，并用不重叠的术语标签在性感的交互式散点图中呈现它们。”\n",
    "\n",
    "然而，在该内核中实现的“CorpusFromParsedDocuments”似乎被弃用或存在依赖性问题，所以我查看了文档并使用了“CorpusFromPandas”，我认为这非常适合我所拥有的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freqs(intent_name):\n",
    "    bagofwords = CountVectorizer()\n",
    "    # Output will be a sparse matrix\n",
    "    inbound = bagofwords.fit_transform(visualize_train[visualize_train['Intent'] == intent_name]['Utterance'])\n",
    "    # Inspecting of often contractions and colloquial language is used\n",
    "    word_counts = np.array(np.sum(inbound, axis=0)).reshape((-1,))\n",
    "    words = np.array(bagofwords.get_feature_names())\n",
    "    words_df = pd.DataFrame({\"word\":words, \n",
    "                                 \"count\":word_counts})\n",
    "    words_rank = words_df.sort_values(by=\"count\", ascending=False)\n",
    "    return words_rank\n",
    "\n",
    "update_df = term_freqs('update')\n",
    "repair_df = term_freqs('repair')\n",
    "\n",
    "combined = pd.concat([update_df, repair_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import scattertext as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data munging\n",
    "visualize_train = neat_train.copy()\n",
    "visualize_train['Utterance'] = visualize_train['Utterance'].progress_apply(\" \".join)\n",
    "\n",
    "# Subsetting to the two intents I want to compare\n",
    "visualize_train = visualize_train[(visualize_train['Intent'] == 'repair') | \n",
    "                                 (visualize_train['Intent'] == 'update')]\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])\n",
    "visualize_train['parsed'] = visualize_train['Utterance'].progress_apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = st.CorpusFromParsedDocuments(visualize_train,\n",
    "                             category_col='Intent',\n",
    "                             parsed_col='parsed').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(corpus,\n",
    "          category='Intent',\n",
    "          category_name='repair',\n",
    "          not_category_name='update',\n",
    "          width_in_pixels=600,\n",
    "          minimum_term_frequency=10,\n",
    "        term_significance = st.LogOddsRatioUninformativeDirichletPrior(),\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1fkySyFbx0j"
   },
   "source": [
    "# Keras的意向分类\n",
    "在我过去的笔记本中，我的目标是为我的聊天机器人接收标记的数据。现在，本笔记本的重点是使用Keras对用户可能键入的新的、看不见的数据的意图进行分类。现在，我们从上一本笔记本中的无监督学习中生成了标签，模型切换到了监督学习方法。\n",
    "\n",
    "### Rasa比较\n",
    "Rasa使用SVM和GridsearchCV训练这一意图分类步骤，因为它们可以尝试不同的配置（[来源](https://medium.com/bhavaniravi/intent-classification-demystifying-rasanlu-part-4-685fc02f5c1d)).部署预处理时，训练和测试之间的管道应保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "uVG4gfRQbx0k",
    "outputId": "b5b06a15-87fd-49c2-c89f-e0c378712c4f",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data science\n",
    "import pandas as pd\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "import numpy as np\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "\n",
    "# Deep Learning \n",
    "import tensorflow as tf\n",
    "print(f\"Tensorflow: {tf.__version__}\")\n",
    "from tensorflow import keras\n",
    "print(f\"Keras: {keras.__version__}\")\n",
    "import sklearn\n",
    "print(f\"Sklearn: {sklearn.__version__}\")\n",
    "\n",
    "# Visualization \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "import collections\n",
    "import yaml\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Preprocessing and Keras\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "\n",
    "# Reading in training data\n",
    "train = pd.read_pickle('objects/train.pkl')\n",
    "print(f'Training data: {train.head()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Muag_xYwbx0p"
   },
   "source": [
    "# Keras Preprocessing\n",
    "\n",
    "### Keras Tokenizer\n",
    "创建vocb中所有单词的字典，并存储索引。对于每个序列，它在序列中传递，并将每个单词转换为引用Keras单词词典的索引。当你把句子输入到模型中时，它们都必须是相同的长度。但有些推文会比其他推文长，所以pad_sequences只会填充所有其他推文，使它们的长度相同。它用0s填充消息，直到它们与最长消息的长度相同。较短的最大长度通常是优选的，因为较长的序列更难训练。\n",
    "\n",
    "我已经完成了大部分主要的预处理工作，但Keras需要一些更具体的东西来进行建模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NF7ZZItmcD_C",
    "outputId": "fa634fec-5282-40b9-d2c2-08c8b13e0931"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "IAjJqgSJbx0p",
    "outputId": "6a8445ed-6d34-44df-da33-79cd9d1e9e16",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "# Label encoding the target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# For the text data\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# I use Keras' Tokenizer API - helpful link I followed: https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n",
    "# Train test split\n",
    "# Split in to train and test (stratify for class imbalance and random state for reproducibility)\n",
    "X_train, X_val, y_train, y_val = train_test_split(train['Utterance'], train['Intent'], test_size = 0.3, \n",
    "                                                   shuffle = True, stratify = train['Intent'], random_state = 7)\n",
    "print(f'\\nShape checks:\\nX_train: {X_train.shape} X_val: {X_val.shape}\\ny_train: {y_train.shape} y_val: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-9-OfyoPfBxt"
   },
   "outputs": [],
   "source": [
    "# Encoding the target variable\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "y_train = le.transform(y_train)\n",
    "y_val = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "unIGcoeIflGa",
    "outputId": "257720ff-1526-405a-ceb9-299ac20073b0"
   },
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "UR2c5gczbx0s",
    "outputId": "a5ce83f2-61e3-488e-d35d-ec3902e7fe79"
   },
   "outputs": [],
   "source": [
    "## 1. ENCODING THE TEXT DATA\n",
    "\n",
    "# NOTE: Since we use an embedding matrix, we use the Tokenizer API to integer encode our data - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(X_train)\n",
    "\n",
    "print(\"Document Count: \\n{}\\n\".format(t.document_count))\n",
    "# print(\"Word index: \\n{} \\n \".format(t.word_index))\n",
    "# print(\"Word Counts: \\n{} \\n\".format(len(t.word_counts) + 1))\n",
    "# print(\"Word docs: \\n{} \\n \".format(t.word_docs))\n",
    "\n",
    "def convert_to_padded(tokenizer, docs):\n",
    "    ''' Taking in Keras API Tokenizer and documents and returns their padded version '''\n",
    "    ## Using API's attributes\n",
    "    # Embedding\n",
    "    embedded = t.texts_to_sequences(docs)\n",
    "    # Padding\n",
    "    padded = pad_sequences(embedded, maxlen = max_length, padding = 'post')\n",
    "    return padded\n",
    "\n",
    "## Defining useful variables for later\n",
    "# Adding 1 becuase of reserved 0 index\n",
    "vocab_size = len(t.word_counts) + 1\n",
    "print(f'Vocab size:\\n{vocab_size}')\n",
    "\n",
    "# Pad documents to a max length\n",
    "max_length = len(max(embedded_X_train, key = len))\n",
    "\n",
    "print(f'Max length:\\n{max_length}')\n",
    "\n",
    "padded_X_train = convert_to_padded(tokenizer = t, docs = X_train)\n",
    "padded_X_val = convert_to_padded(tokenizer = t, docs = X_val)\n",
    "\n",
    "print(f'padded_X_train\\n{padded_X_train}')\n",
    "print(f'padded_X_val\\n{padded_X_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vwDfMawtbx0v",
    "outputId": "256b279d-4f8d-4464-fef7-ea4e54937b8b"
   },
   "outputs": [],
   "source": [
    "padded_X_train.shape, padded_X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "HkqW3VjKqwKI",
    "outputId": "6ab620ec-f1db-4b8e-ac26-26c14102e5b3"
   },
   "outputs": [],
   "source": [
    "padded_X_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xH0LDPxGbx0y"
   },
   "source": [
    "运行此示例适合使用5个小文档的Tokenizer。将打印fit Tokenizer的详细信息。然后使用字数对这5个文档进行编码。\n",
    "\n",
    "每个文档被编码为9元素矢量，每个单词具有一个位置，每个单词位置具有所选择的编码方案值。在这种情况下，使用简单的单词计数模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0BJGJ_6jbx0z"
   },
   "source": [
    "### Embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_gU9p1Gbx0z"
   },
   "source": [
    "Keras模型寻找一个热编码的y变量。当它是多类的时候，很多人把它当作一个热门的编码向量。这只是设计选择之一。\n",
    "\n",
    "\n",
    "如果你使用的是doc2vec嵌入，你如何传入你的推文。您可能需要将其作为完整的推文传递。看看你是如何在推特上传递的。你可能需要在推特级别进行标记。如果你把它传进来，如果它是Tweet 57，它会激活节点，使它与第57个文档的嵌入相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vif1Y7qqbx00",
    "outputId": "40230d4e-46ba-4273-ecce-9ac0e20b299a"
   },
   "outputs": [],
   "source": [
    "# We can see that there are 4 different dimensionality options\n",
    "!ls models/glove.twitter.27B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T83TIrvPbx03"
   },
   "source": [
    "Here, we compute an index mapping words to known embeddings by parsing the data dump of pre-trained embeddings:\n",
    "\n",
    "I use 50D because my X_train has a max_length of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2LpoFpsDbx03",
    "outputId": "992de14f-94d7-4fd6-b414-5d815c348b1d"
   },
   "outputs": [],
   "source": [
    "# Using gloVe word embeddings\n",
    "embeddings_index = {}\n",
    "f = open('models/glove.twitter.27B/glove.twitter.27B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NkBbmMOkbx06"
   },
   "source": [
    "Now we can leverage our embedding_index dictionary and our word_index to compute our embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4tdN1wuLbx07"
   },
   "outputs": [],
   "source": [
    "# Initializing required objects\n",
    "word_index = t.word_index\n",
    "EMBEDDING_DIM = 50 # Because we are using the 50D gloVe embeddings\n",
    "\n",
    "# Getting my embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "rZXsRtHTbx0-",
    "outputId": "8dc559cd-665e-4645-a19c-b6a265b6df52"
   },
   "outputs": [],
   "source": [
    "embedding_matrix, embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4HBkKIPbx1C"
   },
   "source": [
    "太好了，现在我们可以开始建模了。\n",
    "\n",
    "在常规单词嵌入中，必须设置矩阵中嵌入的顺序，使其与单词在我的keras标记器单词索引中的显示方式相匹配。它这样做是为了让最常见的单词出现在前面，并且嵌入矩阵需要对齐。\n",
    "\n",
    "我还确保嵌入的顺序与我的模型中单词的顺序相同。\n",
    "\n",
    "在这里，我还确保像macbook这样的特定领域的单词在我的Twitter嵌入中。其中一个例子是“macbook”，你可以清楚地看到它确实在embeddings文件中，这很好：\n",
    "\n",
    "<img src=\"visualizations/macbook-glove.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d7khIrmGbx1C"
   },
   "source": [
    "# Keras Modelling\n",
    "I will create a neural network with Keras with the output layer having the same number of nodes as there are intents. The following is my architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "cVOOkDNdbx1F",
    "outputId": "92f75bea-daa4-462d-8811-3546d28c3501",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_model(vocab_size, max_token_length):\n",
    "    ''' In this function I define all the layers of my neural network'''\n",
    "    # Initialize\n",
    "    model = Sequential()\n",
    "    #model.add(Input(shape = (32,), dtype = 'int32'))\n",
    "\n",
    "    # Adding layers - For embedding layer, I made sure to add my embedding matrix into the weights paramater\n",
    "    model.add(Embedding(vocab_size, embedding_matrix.shape[1], input_length = 32, \n",
    "                        trainable = False, weights = [embedding_matrix]))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "#    model.add(LSTM(128)) \n",
    "    # Try 100\n",
    "    model.add(Dense(600, activation = \"relu\",kernel_regularizer ='l2')) # Try 50, another dense layer? This takes a little bit of exploration\n",
    "    \n",
    "    # Adding another dense layer to increase model complexity\n",
    "    model.add(Dense(600, activation = \"relu\",kernel_regularizer ='l2'))\n",
    "    \n",
    "    # Only update 50 percent of the nodes - helps with overfitting\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # This last layer should be the size of the number of your intents!\n",
    "    # Use sigmoid for multilabel classification, otherwise, use softmax!\n",
    "    model.add(Dense(10, activation = \"softmax\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Actually creating my model with 32 as the max token length\n",
    "model = make_model(vocab_size, 32)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", \n",
    "              optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "colab_type": "code",
    "id": "gnq7SY7vbx1I",
    "outputId": "150c2ce0-e9d9-493f-9bfc-674b1d8c73ef"
   },
   "outputs": [],
   "source": [
    "# Initializing checkpoint settings to view progress and save model\n",
    "filename = 'models/intent_classification_b.h5'\n",
    "\n",
    "# Learning rate scheduling\n",
    "# This function keeps the initial learning rate for the first ten epochs  \n",
    "# and decreases it exponentially after that.  \n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "lr_sched_checkpoint = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# This saves the best model\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min')\n",
    "\n",
    "# The model you get at the end of it is after 100 epochs, but that might not have been\n",
    "# the weights most associated with validation accuracy\n",
    "\n",
    "# Only save the weights when you model has the lowest val loss. Early stopping\n",
    "\n",
    "# Fitting model with all the callbacks above\n",
    "hist = model.fit(padded_X_train, y_train, epochs = 20, batch_size = 32, \n",
    "                 validation_data = (padded_X_val, y_val), \n",
    "                 callbacks = [checkpoint, lr_sched_checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmiVKysabx1K"
   },
   "source": [
    "注意：对于任何新的测试数据，它必须采用完全相同的格式。因此，如果您对已经预标记的文档调用fit_to_texts，那么您传入的字符串也必须作为预标记字符串传入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "colab_type": "code",
    "id": "HALampOEbx1L",
    "outputId": "9c206dbc-f97c-468b-8b4c-aa1a1553c61b"
   },
   "outputs": [],
   "source": [
    "# Visualizing Training Loss vs Validation Loss (the loss is how wrong your model is)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(hist.history['val_loss'], label = 'Validation Loss', color = 'cyan')\n",
    "plt.plot(hist.history['loss'], label = 'Training Loss', color = 'purple')\n",
    "plt.title('Training Loss vs Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualizing Testing Accuracy vs Validation Accuracy\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(hist.history['val_accuracy'], label = 'Validation Accuracy', color = 'cyan')\n",
    "plt.plot(hist.history['accuracy'], label = 'Training Accuracy', color = 'purple')\n",
    "plt.title('Training Accuracy vs Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CkcVbdNubx1O"
   },
   "source": [
    "在20个时期之后，坡度变成了一条平坦的线，损失并没有太大变化。地板效应是你不能得到任何低于0的损失。它真的很快就从训练数据中学习到了它需要学习的东西。如果你继续训练，你基本上是在过度适应训练数据，你在适应不重要的信号。\n",
    "\n",
    "例如，在图像的背景下，如果模型学会了识别猫是什么，它现在可能过于详细，并学会了猫也必须是黑色。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGqGvxRobx1O"
   },
   "source": [
    "### Model improvements\n",
    "该模型在低时期拟合过度。模型明显过拟合。绘制出准确度。\n",
    "\n",
    "不需要100次训练。\n",
    "\n",
    "看学习率调度，在一定数量的历元之后，降低学习率。\n",
    "* Learning rate scheduling\n",
    "* Early stopping or reducing epochs\n",
    "* Dropout layers\n",
    "* Regularization\n",
    "* Improve distinctiveness between intent data\n",
    "\n",
    "在我应用了这些改进之后，我的准确性提高了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7SYdTawRbx1P"
   },
   "outputs": [],
   "source": [
    "# I have to redefine and load in the model saved by my model checkpoint \n",
    "from keras.models import load_model\n",
    "model = load_model('models/intent_classification_b.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_intent(user_input):\n",
    "    ''' Making a function that recieves a user input and outputs a \n",
    "    dictionary of predictions '''\n",
    "    assert isinstance(user_input, str), 'User input must be a string!'\n",
    "    user_input = [user_input]\n",
    "    print(user_input)\n",
    "    \n",
    "    # Converting to Keras form\n",
    "    padded_text = convert_to_padded(t, user_input)\n",
    "    x = padded_text[0]\n",
    "    \n",
    "    # Prediction for each document\n",
    "    probs = model.predict(padded_text)\n",
    "#     print('Prob array shape', probs.shape)\n",
    "    \n",
    "    # Get the classes from label encoder\n",
    "    classes = le.classes_\n",
    "    \n",
    "    # Getting predictions dict and sorting\n",
    "    predictions = dict(zip(classes, probs[0]))\n",
    "    sorted_predictions = {k: v for k, v in sorted(predictions.items(), key=lambda item: item[1], reverse = True)}\n",
    "    \n",
    "    return sorted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_intent('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我花了大量的时间来完善我输入到这个模型中的训练数据，尤其是试图找出这个模型的正确映射。最终修复我的映射的是使用标签编码器，而不是为我的目标变量使用一个热编码器，并确保我的用户输入格式正确（它应该是一个列表，因为它通过了维度检查）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VtrsTXPQjgT8",
    "outputId": "704ecaec-d0ba-4052-f529-27d74bfd3460"
   },
   "outputs": [],
   "source": [
    "classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "VoY8RpgVbx1c",
    "outputId": "da6b9c33-6a34-4017-865b-fbbbfbd8a3c0"
   },
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0pkL2XRrbx1e",
    "outputId": "2317110d-d84c-47f1-8567-d2269d0d7172"
   },
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "clS2z_gUrzWH",
    "outputId": "6acab942-d414-4e7a-c272-40bd32f6c7c3"
   },
   "outputs": [],
   "source": [
    "padded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "5AbFpXLIsWCz",
    "outputId": "5f5ca0e2-fbc8-45c9-aed8-adbd62d1e96c"
   },
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pq6VhkOrsfTh"
   },
   "outputs": [],
   "source": [
    "test = [X_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lMr-5Ksbsda5"
   },
   "outputs": [],
   "source": [
    "embedded_text = t.texts_to_sequences(test)\n",
    "\n",
    "padded_text = pad_sequences(embedded_text, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NWWM3gsispn_",
    "outputId": "31c0884a-42dd-4a1c-b07c-dde533303ac4"
   },
   "outputs": [],
   "source": [
    "embedded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "2MBKjjqduij9",
    "outputId": "a2c00226-bf09-4b92-8485-8ca8eb9d0dff"
   },
   "outputs": [],
   "source": [
    "padded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1s6sm5SsuUBO",
    "outputId": "86dcfd22-4ee3-427d-d779-0c41e30f4736"
   },
   "outputs": [],
   "source": [
    "t.word_index['battery']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xlrZfIjfbx1g"
   },
   "source": [
    "# Future Step: Multilabel Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnlO0KQTbx1g"
   },
   "source": [
    "将来，如果我想识别话语中的混合意图，我可以进行多标签分类。\n",
    "\n",
    "对于多标签分类，使用sigmoid激活函数作为最后一层。你仍然会有大约10个不同的意图。但你需要建模，使这些意图中的每一个都相互独立。\n",
    "意图1的预测不应影响意图2。Softmax获取所有类的所有分数，最高的数字将具有最高的概率输出，但所有的总和将为1。对于最终的softmax层，总和将为1，但这在我的情况下不起作用。\n",
    "\n",
    "但你要分别对每个意图进行分类。它们的总和可以大于1。\n",
    "\n",
    "它类似于多类中的logreg。一条曲线用于类0而非类0。这些问题的总和可以大于1。\n",
    "\n",
    "对于类1，它将是1或不是1。等等。你可以看看你的输出层，无论哪个节点的概率输出大于0.5，这2个都是你的最终输出。你最多可以做3个。取决于您将拥有多少个节点。\n",
    "\n",
    "当你输入你的目标向量时，它们需要进入一个热门的编码向量。目标列将有10列。对于每个节点，这一切加起来就是一个。每个节点将具有单独的S形函数（P（1-P））。在节点之间，它们的总和将超过1。一对一分类。根据logreg条款进行阅读。多标签分类。最重要的是你的标签需要一个热编码。损失函数将使用二进制交叉熵。\n",
    "\n",
    "潜在问题：你的类越多，你的模型就越难。尤其是对于第二和第三个标签，这是acc开始下降的时候。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
